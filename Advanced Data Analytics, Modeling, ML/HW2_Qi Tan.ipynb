{"cells":[{"cell_type":"markdown","metadata":{"id":"lNpOrh_9BczA"},"source":["# QM 701: Advanced Data Analytics and Applications\n","# Homework 2\n","\n","\n","-------\n","\n","## Objective\n","The goal of this homework is to help you further familiarize with the concept of sentiment analysis and its application piplines. In this homework, you will delve into the fundamental principles of sentiment analysis, explore various techniques, and gain hands-on experience by implementing sentiment analysis on real-world text data.\n","\n","## Tasks\n","This homework includes the following 6 questions.\n"," - **Q1**: Labels. Map the initial emotions to the simpler categories. (10 points)\n"," - **Q2**: Text Pre-processing. How would you pre-process the text to clean the data for sentiment analysis? (20 points)\n"," - **Q3**: Vectorization: Convert the text to data using BoW or TF-IDF using the sklearn library (CountVectorizer or TfidfVectorizer). You will also need to split the data into training/test in this step. (20 points)\n"," - **Q4**: Naive Bayes Model: Use Naive-Bayes (MultinomialNB) within sklearn to train your model and test your results (20 points)\n"," - **Q5**: Multi-Categorical Logistic Regression: Use LogisticRegression (Multinomial Logistic Regression) within sklearn to train your model and test your results (15 points)\n"," - **Q6**: Apply the TextBlob sentiment analysis library to the same data set. Compare the results with the Naive Bayes model. (15 points + 5 Bonus Points)"]},{"cell_type":"markdown","metadata":{"id":"odOv_-nYCNS5"},"source":["**Files required**:\n","*  english_stopwords\n","*  text_classification dataset\n","\n","**Source for Corpus**:\n","*  https://data.world/crowdflower/sentiment-analysis-in-text\n","*  Author: [@CrowdFlower](https://data.world/crowdflower)\n","*  Direct download link via Box: https://duke.box.com/s/phdcgpx2bgbrrg0ydiw7l05837q82c2y\n","\n","**Homework topics covered**:\n","1.  Text Vectorization (Converting text to numbers)\n","2.  Text Pre-processing\n","3.  Text Classification\n","4.  Applied Sentiment Analysis\n","5.  Python for NLP and Machine Learning\n","\n","**Colab Note**:\n","Don't forget you can click the > arrows next to topics to expand/hide sections of code within the notebook."]},{"cell_type":"markdown","metadata":{"id":"b5V4gLPLs1_y"},"source":["## File and Data setup\n","\n","*  Loads stopwords: *stopwords*\n","*  Loads a dataframe: *emotion_raw_csv*"]},{"cell_type":"code","execution_count":122,"metadata":{"id":"SHwBSBBds4lU","executionInfo":{"status":"ok","timestamp":1719461538140,"user_tz":240,"elapsed":170,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}}},"outputs":[],"source":["# General dataframe imports\n","import pandas as pd\n","import numpy as np\n","\n","# sklearn imports\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn import naive_bayes\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction import text\n","import sklearn.feature_extraction\n","\n","# Stopwords\n","import nltk\n","\n","# Sentiment Analysis\n","from textblob import TextBlob\n","\n","# Regular Expressions\n","import re\n","\n","# Preprocessing\n","import gensim"]},{"cell_type":"code","execution_count":123,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R8Rp89v9uQJY","outputId":"af4f2064-6e67-464d-e8ce-52f3eb2a419a","executionInfo":{"status":"ok","timestamp":1719461538348,"user_tz":240,"elapsed":2,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["# Download Stopwords\n","# Downloads stopwords to /root/nltk_data/corpora/stopwords/english\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","# Import stopwords to a list:\n","sfile = open(stopwords._root.path + '/english','r')\n","stopwords = sfile.read().splitlines()\n","sfile.close()"]},{"cell_type":"code","execution_count":124,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6OGtGhtJuTXZ","outputId":"820b8fc2-6352-4493-be24-1547f4270f7d","executionInfo":{"status":"ok","timestamp":1719461539690,"user_tz":240,"elapsed":1344,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-06-27 04:17:31--  https://duke.box.com/shared/static/ll2bqmkdxsnj8wmm6zmqwlmct2hbf7oe\n","Resolving duke.box.com (duke.box.com)... 74.112.186.144\n","Connecting to duke.box.com (duke.box.com)|74.112.186.144|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /public/static/ll2bqmkdxsnj8wmm6zmqwlmct2hbf7oe [following]\n","--2024-06-27 04:17:31--  https://duke.box.com/public/static/ll2bqmkdxsnj8wmm6zmqwlmct2hbf7oe\n","Reusing existing connection to duke.box.com:443.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://duke.app.box.com/public/static/ll2bqmkdxsnj8wmm6zmqwlmct2hbf7oe [following]\n","--2024-06-27 04:17:31--  https://duke.app.box.com/public/static/ll2bqmkdxsnj8wmm6zmqwlmct2hbf7oe\n","Resolving duke.app.box.com (duke.app.box.com)... 74.112.186.144\n","Connecting to duke.app.box.com (duke.app.box.com)|74.112.186.144|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://public.boxcloud.com/d/1/b1!WyV1G1s23uKEc7ZRfhEHGmJDisxRmaaHNH6YshpezL8jXqCZZALG1H4H36a2nZf2OgxqcdpNCPDuoS-1geB2srBkuhSO9CxTuMNazkAXp2v1vno_XfpIpEUzEUXz0HfVAMBaXo1hkgwzfOpwFPaf6y6WW3gO3b1ws1kqwoaZJ4StnbkSwHe1B8LfYlhG_8KMh4c6bF1Xzlf1TVqhS_gjj1BbowFHZpT3btdSciJUxsBgNxVQryNb5QRMsXmSvJ6upTOWxm2eJPUJ7i3o_9A6oiYrwQ5cI7pt54fASQPHcUf9b69c8DM2fnyI_6w7WwONXxSlIYQCtboCeQmBu5hkKZxMOUVsR6GtoAqCuqhWNqD9_pigUhN0m581Tl9KZ8Xi9igz8QbyN1Czrv3dC_0siSpyqgM4jwJYwmNbdjHoMQbLoATd3A4RRLGTILCnx2965hTRS9iQ939c38G3DI2Tn4BeTAlI9WYeIqArtw2acl7BF6NkytcgLNoGjo6Uy9gH2XafRJKs6baS1-1WiTymy537pVr9nVYJk0hykYwRngIqnpzHs4qEAotZbzfU-LQeAyhKKebIopoPumu6xQDhUlm_-MLqNQ4p3Z7e1Nr2WVdCnIldCBJhBsr562UUxby2Q9utLbNZBeiiOTMN1i0NbgxkZ0KViSNsYc3sDaaIg2_b9IY4xmqokzu6YOxaTviOEPZYBzyDXEgt-LgWxPQEhAjJkEj5QBYLlc2fB7izlFL8vqQGAbKYF8Ju5xwAWMaDpQBBaFQr_dbpmjiDTM_jzznLtOZGqjmxiva0daxPcjFxbVi2cKx8UCw2ktHMv5hWSQ-T6NZ6ML_W3E2p2ZUntvePTha4vwrtHX3YK83P4Bjuux9ZEsbY4HVzLla7PGzFaKgCGK2f0Qz6IBTrmKRxfDJ95jItb5ArgXwLLurmN7NihUQMQgMR9l3J0auh7e2QH17MD2iEEGjhCHpckEMe0j7W38D-LgeVEyhRtE_3vRhI50I8BgK4bljZiCbPEEhyK-ujmo6ABZrpR5h9_3oUtt53C5j_Ho0Yojjiy2W-VV0gjluUrCxi7vxWEeA0clW7ilwabqShQmkG3MIwl1eYCKNnN82uih0XRmv10WpA7d-zg8VEvmGMQPppa3eDXsKgtndDWcq5lePXoNc_caInHqbDwLpBedzcOOOxfHfcCRV2kmnL6lSfIMKt_yGWfzZRNf6_P1wDbUEZyMuKDoMoW3OIqtUpzOBa_pHsVjkvXTJJb-dW6syi2WMGIF78Uhxne1GaDVMPJkQs8erfKzCGdXdCwwI_0iSJ62FAg47-Ol9PcYbO--QaGQjl6l_KUiHF1jGZzHfzLMkjHS9b6JegX4LmY7friERcEkRf/download [following]\n","--2024-06-27 04:17:32--  https://public.boxcloud.com/d/1/b1!WyV1G1s23uKEc7ZRfhEHGmJDisxRmaaHNH6YshpezL8jXqCZZALG1H4H36a2nZf2OgxqcdpNCPDuoS-1geB2srBkuhSO9CxTuMNazkAXp2v1vno_XfpIpEUzEUXz0HfVAMBaXo1hkgwzfOpwFPaf6y6WW3gO3b1ws1kqwoaZJ4StnbkSwHe1B8LfYlhG_8KMh4c6bF1Xzlf1TVqhS_gjj1BbowFHZpT3btdSciJUxsBgNxVQryNb5QRMsXmSvJ6upTOWxm2eJPUJ7i3o_9A6oiYrwQ5cI7pt54fASQPHcUf9b69c8DM2fnyI_6w7WwONXxSlIYQCtboCeQmBu5hkKZxMOUVsR6GtoAqCuqhWNqD9_pigUhN0m581Tl9KZ8Xi9igz8QbyN1Czrv3dC_0siSpyqgM4jwJYwmNbdjHoMQbLoATd3A4RRLGTILCnx2965hTRS9iQ939c38G3DI2Tn4BeTAlI9WYeIqArtw2acl7BF6NkytcgLNoGjo6Uy9gH2XafRJKs6baS1-1WiTymy537pVr9nVYJk0hykYwRngIqnpzHs4qEAotZbzfU-LQeAyhKKebIopoPumu6xQDhUlm_-MLqNQ4p3Z7e1Nr2WVdCnIldCBJhBsr562UUxby2Q9utLbNZBeiiOTMN1i0NbgxkZ0KViSNsYc3sDaaIg2_b9IY4xmqokzu6YOxaTviOEPZYBzyDXEgt-LgWxPQEhAjJkEj5QBYLlc2fB7izlFL8vqQGAbKYF8Ju5xwAWMaDpQBBaFQr_dbpmjiDTM_jzznLtOZGqjmxiva0daxPcjFxbVi2cKx8UCw2ktHMv5hWSQ-T6NZ6ML_W3E2p2ZUntvePTha4vwrtHX3YK83P4Bjuux9ZEsbY4HVzLla7PGzFaKgCGK2f0Qz6IBTrmKRxfDJ95jItb5ArgXwLLurmN7NihUQMQgMR9l3J0auh7e2QH17MD2iEEGjhCHpckEMe0j7W38D-LgeVEyhRtE_3vRhI50I8BgK4bljZiCbPEEhyK-ujmo6ABZrpR5h9_3oUtt53C5j_Ho0Yojjiy2W-VV0gjluUrCxi7vxWEeA0clW7ilwabqShQmkG3MIwl1eYCKNnN82uih0XRmv10WpA7d-zg8VEvmGMQPppa3eDXsKgtndDWcq5lePXoNc_caInHqbDwLpBedzcOOOxfHfcCRV2kmnL6lSfIMKt_yGWfzZRNf6_P1wDbUEZyMuKDoMoW3OIqtUpzOBa_pHsVjkvXTJJb-dW6syi2WMGIF78Uhxne1GaDVMPJkQs8erfKzCGdXdCwwI_0iSJ62FAg47-Ol9PcYbO--QaGQjl6l_KUiHF1jGZzHfzLMkjHS9b6JegX4LmY7friERcEkRf/download\n","Resolving public.boxcloud.com (public.boxcloud.com)... 74.112.186.128\n","Connecting to public.boxcloud.com (public.boxcloud.com)|74.112.186.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4394791 (4.2M) [text/csv]\n","Saving to: ‘text_emotion.csv’\n","\n","text_emotion.csv    100%[===================>]   4.19M  --.-KB/s    in 0.08s   \n","\n","2024-06-27 04:17:32 (54.0 MB/s) - ‘text_emotion.csv’ saved [4394791/4394791]\n","\n"]}],"source":["# Download the working dataset\n","# saves a file to your directory as /content/text_emotion.csv\n","!wget https://duke.box.com/shared/static/ll2bqmkdxsnj8wmm6zmqwlmct2hbf7oe -O text_emotion.csv"]},{"cell_type":"code","execution_count":125,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4vTEGNhdu-Zm","outputId":"64e3a769-db23-4d3c-f799-e7ac48f7553a","executionInfo":{"status":"ok","timestamp":1719461539898,"user_tz":240,"elapsed":209,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["(40000, 4)\n"]}],"source":["import os\n","file_path = os.path.join(os.getcwd(), 'text_emotion.csv')\n","\n","# Use pandas to read the text_emotion_file into memory\n","emotion_raw_csv = pd.read_csv(file_path)\n","\n","# Print out the shape of emotion_raw_csv\n","print(emotion_raw_csv.shape)"]},{"cell_type":"code","execution_count":126,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"WOyUSiUrvbDW","outputId":"18892cbd-1b12-4cea-9a58-a85efd6ffa11","executionInfo":{"status":"ok","timestamp":1719461540082,"user_tz":240,"elapsed":185,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["     tweet_id   sentiment       author  \\\n","0  1956967341       empty   xoshayzers   \n","1  1956967666     sadness    wannamama   \n","2  1956967696     sadness    coolfunky   \n","3  1956967789  enthusiasm  czareaquino   \n","4  1956968416     neutral    xkilljoyx   \n","\n","                                             content  \n","0  @tiffanylue i know  i was listenin to bad habi...  \n","1  Layin n bed with a headache  ughhhh...waitin o...  \n","2                Funeral ceremony...gloomy friday...  \n","3               wants to hang out with friends SOON!  \n","4  @dannycastillo We want to trade with someone w...  "],"text/html":["\n","  <div id=\"df-3c6c1117-382f-4d35-abcb-0a6ce0565076\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>sentiment</th>\n","      <th>author</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1956967341</td>\n","      <td>empty</td>\n","      <td>xoshayzers</td>\n","      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1956967666</td>\n","      <td>sadness</td>\n","      <td>wannamama</td>\n","      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1956967696</td>\n","      <td>sadness</td>\n","      <td>coolfunky</td>\n","      <td>Funeral ceremony...gloomy friday...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1956967789</td>\n","      <td>enthusiasm</td>\n","      <td>czareaquino</td>\n","      <td>wants to hang out with friends SOON!</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1956968416</td>\n","      <td>neutral</td>\n","      <td>xkilljoyx</td>\n","      <td>@dannycastillo We want to trade with someone w...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c6c1117-382f-4d35-abcb-0a6ce0565076')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3c6c1117-382f-4d35-abcb-0a6ce0565076 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3c6c1117-382f-4d35-abcb-0a6ce0565076');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-fe287011-144f-498e-a76d-93c1cb5f38b6\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fe287011-144f-498e-a76d-93c1cb5f38b6')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-fe287011-144f-498e-a76d-93c1cb5f38b6 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"emotion_raw_csv","summary":"{\n  \"name\": \"emotion_raw_csv\",\n  \"rows\": 40000,\n  \"fields\": [\n    {\n      \"column\": \"tweet_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 118857896,\n        \"min\": 1693956175,\n        \"max\": 1966441171,\n        \"num_unique_values\": 40000,\n        \"samples\": [\n          1752414968,\n          1965295852,\n          1696219218\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          \"relief\",\n          \"happiness\",\n          \"empty\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33871,\n        \"samples\": [\n          \"AmberLovesNKOTB\",\n          \"HeartPanda\",\n          \"deucehartley\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 39827,\n        \"samples\": [\n          \"the sun is shinning! im off out!!\",\n          \"wow i must have been tired. i fell asleep @ exactly the start of the 10pm news &amp; now up as if its 8am.\",\n          \"@knobzie their loss\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":126}],"source":["# Peek at the dataframe\n","emotion_raw_csv.head()"]},{"cell_type":"markdown","metadata":{"id":"BAZJOO3VCVr-"},"source":["## Q1: **Labels**\n","Since there are multiple emotions in the initial file, you need to map the initial emotions to the simpler categories.\n"]},{"cell_type":"code","execution_count":127,"metadata":{"id":"jMf_WmIiEICm","executionInfo":{"status":"ok","timestamp":1719461540082,"user_tz":240,"elapsed":1,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}}},"outputs":[],"source":["# We have 13 emotions in this text file:\n","emotions_list = ['empty','sadness','enthusiasm','neutral','worry',\n","          'surprise','love','fun','hate','happiness','boredom',\n","          'relief','anger']"]},{"cell_type":"markdown","metadata":{"id":"ob4EDSPVEie2"},"source":["### a) Decide your model forms\n","\n","Please indicate whehter you want to select a model with:\n","*  multi-categorical labels between positive, negative, and neutral\n","or\n","*  binary labels between positive and negative.\n","\n","\n","Enter your choice, along with a short explanation:\n"]},{"cell_type":"markdown","metadata":{"id":"Y-pVaDUg49a2"},"source":["**Answer: **I would choose mult-categorical labels since they are more comprehensive to include many different types of emotions"]},{"cell_type":"markdown","metadata":{"id":"1j3C4mUtFUJU"},"source":["### b) Map the emotions into the 'y' labels. (Note: be sure to review the code below and make the changes if needed)"]},{"cell_type":"code","execution_count":128,"metadata":{"id":"QUHDtjCtFuO7","executionInfo":{"status":"ok","timestamp":1719461540268,"user_tz":240,"elapsed":187,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}}},"outputs":[],"source":["# Based on your choice of the model form,\n","# re-assign neutral (0) (if applicable), positive (1),\n","# and negative (-1) to the 13 emotions by changing the following dict values\n","\n","emotions_dict = {\n","    'empty': -1,\n","    'sadness': -1,\n","    'enthusiasm': 1,\n","    'neutral': 0,\n","    'worry': -1,\n","    'surprise': 0,\n","    'love': 1,\n","    'fun': 1,\n","    'hate': -1,\n","    'happiness': 1,\n","    'boredom': -1,\n","    'relief': 1,\n","    'anger': -1\n","    }"]},{"cell_type":"markdown","metadata":{"id":"b_AS1B3oWKNM"},"source":["### c) Briefly discuss or comments as appropriate for your decision on the model form and the emotion mappings:"]},{"cell_type":"markdown","metadata":{"id":"pa_DqByvKNO_"},"source":["**Answer: **. I chose multi-categorical labels for deeper insights and more accurate sentiment representation where neutral plays a role, because we have so many emotions that are actually ambiguous and neutral.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"axS9SooYGLFb"},"source":["### d) Create the 'y' column in the dataframe.\n","We provide the codes for creating the 'y' column. You can just run it without any edits needed."]},{"cell_type":"code","execution_count":129,"metadata":{"id":"Iat8g_g8Eakc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719461540268,"user_tz":240,"elapsed":2,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}},"outputId":"fdf1ffb5-75b6-46e3-c73b-77171e2b23da"},"outputs":[{"output_type":"stream","name":"stdout","text":["     tweet_id   sentiment         author  \\\n","0  1956967341       empty     xoshayzers   \n","1  1956967666     sadness      wannamama   \n","2  1956967696     sadness      coolfunky   \n","3  1956967789  enthusiasm    czareaquino   \n","4  1956968416     neutral      xkilljoyx   \n","5  1956968477       worry  xxxPEACHESxxx   \n","6  1956968487     sadness       ShansBee   \n","7  1956968636       worry       mcsleazy   \n","8  1956969035     sadness    nic0lepaula   \n","9  1956969172     sadness     Ingenue_Em   \n","\n","                                             content  y  \n","0  @tiffanylue i know  i was listenin to bad habi... -1  \n","1  Layin n bed with a headache  ughhhh...waitin o... -1  \n","2                Funeral ceremony...gloomy friday... -1  \n","3               wants to hang out with friends SOON!  1  \n","4  @dannycastillo We want to trade with someone w...  0  \n","5  Re-pinging @ghostridah14: why didn't you go to... -1  \n","6  I should be sleep, but im not! thinking about ... -1  \n","7               Hmmm. http://www.djhero.com/ is down -1  \n","8            @charviray Charlene my love. I miss you -1  \n","9         @kelcouch I'm sorry  at least it's Friday? -1  \n"]}],"source":["# Mapping the 'sentiment' column values using the emotions_dict\n","# This will create a new column 'y' in the DataFrame with the mapped values\n","emotion_raw_csv['y'] = emotion_raw_csv['sentiment'].map(emotions_dict)\n","\n","# Displaying the first 10 rows of the updated DataFrame\n","print(emotion_raw_csv.head(10))\n","# You can then form a 'y' variable as\n","y = emotion_raw_csv['y']"]},{"cell_type":"markdown","metadata":{"id":"eX9x6YOfGwdk"},"source":["## Q2: **Text Pre-Processing**\n"]},{"cell_type":"markdown","metadata":{"id":"tyqOW4cGH997"},"source":["### a) Describe and identify as many preprocessing steps for the corpus `emotion_raw_csv['content']` as you can."]},{"cell_type":"markdown","metadata":{"id":"KCBFYosLHrIw"},"source":["**Answer:** We can perform the following preprocessing:<br>\n","Lowercasing, Removing Punctuation, Number Removal, Whitespaces Removal.<br>\n","Tokenization, remove Stop words, Stemming and Lemmatization, replacing \"isn't\" with \"is_not\", remove Special characters and non-ASCII characters, Rare Words/Token Removal, Synonyms Consolidation and Dependency Parsing."]},{"cell_type":"markdown","metadata":{"id":"ih9LEIFEFJi2"},"source":["### b) Select at least three text preprocessing steps and apply them on the corpus `emotion_raw_csv['content']`.\n","\n","* Please store the processed text as `emotion_raw_csv['processed_content']`.\n","* Ensure that each entry in `emotion_raw_csv['processed_content']` is a string, not a list of tokens.\n","\n","Hint: See `PreClass2_Preprocess_SocialMedia.ipynb` posted on Canvas"]},{"cell_type":"code","execution_count":130,"metadata":{"id":"WPi1SKR1I1DD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719461569663,"user_tz":240,"elapsed":29396,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}},"outputId":"7e703e50-24e6-42b2-d636-a16bced0885a"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["                                                 content  \\\n","0      @tiffanylue i know  i was listenin to bad habi...   \n","1      Layin n bed with a headache  ughhhh...waitin o...   \n","2                    Funeral ceremony...gloomy friday...   \n","3                   wants to hang out with friends SOON!   \n","4      @dannycastillo We want to trade with someone w...   \n","...                                                  ...   \n","39995                                   @JohnLloydTaylor   \n","39996                     Happy Mothers Day  All my love   \n","39997  Happy Mother's Day to all the mommies out ther...   \n","39998  @niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...   \n","39999  @mopedronin bullet train from tokyo    the gf ...   \n","\n","                                       processed_content  \n","0      tiffanylu know listenin bad habit earlier star...  \n","1                  layin n bed headach ughhhhwaitin call  \n","2                            funer ceremonygloomi friday  \n","3                                  want hang friend soon  \n","4      dannycastillo want trade someon houston ticket...  \n","...                                                  ...  \n","39995                                    johnlloydtaylor  \n","39996                              happi mother day love  \n","39997  happi mother day mommi woman man long your mom...  \n","39998  niariley wassup beauti follow peep new hit sin...  \n","39999  mopedronin bullet train tokyo gf visit japan s...  \n","\n","[40000 rows x 2 columns]\n"]}],"source":["# Lowercasing, Removing punctuation, Removing stop words, Stemmer and Lemmatizer:\n","import string\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","import nltk\n","\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","# Lowercasing\n","emotion_raw_csv['processed_content'] = emotion_raw_csv['content'].str.lower()\n","\n","# Removing punctuation\n","emotion_raw_csv['processed_content'] = emotion_raw_csv['processed_content'].apply(\n","    lambda text: text.translate(str.maketrans('', '', string.punctuation))\n",")\n","\n","# Removing stop words\n","emotion_raw_csv['processed_content'] = emotion_raw_csv['processed_content'].apply(\n","    lambda text: ' '.join([word for word in word_tokenize(text) if word not in stopwords])\n",")\n","\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","\n","# Stemming, Lemmatization, and Removing digits\n","emotion_raw_csv['processed_content'] = emotion_raw_csv['processed_content'].apply(\n","    lambda text: ' '.join([stemmer.stem(word) for word in word_tokenize(text.lower()) if word.isalpha()])\n",")\n","\n","emotion_raw_csv['processed_content'] = emotion_raw_csv['processed_content'].apply(\n","    lambda text: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(text)])\n",")\n","\n","# Removing digits\n","emotion_raw_csv['processed_content'] = emotion_raw_csv['processed_content'].apply(\n","    lambda text: re.sub(r'\\d+', '', text)\n",")\n","# Display the DataFrame to check results\n","print(emotion_raw_csv[['content', 'processed_content']])\n"]},{"cell_type":"code","execution_count":131,"metadata":{"id":"2gZHzpxRY85u","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1719461569874,"user_tz":240,"elapsed":219,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}},"outputId":"f8533fa4-4325-45bc-b2c5-34cb4d736425"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["     tweet_id   sentiment       author  \\\n","0  1956967341       empty   xoshayzers   \n","1  1956967666     sadness    wannamama   \n","2  1956967696     sadness    coolfunky   \n","3  1956967789  enthusiasm  czareaquino   \n","4  1956968416     neutral    xkilljoyx   \n","\n","                                             content  y  \\\n","0  @tiffanylue i know  i was listenin to bad habi... -1   \n","1  Layin n bed with a headache  ughhhh...waitin o... -1   \n","2                Funeral ceremony...gloomy friday... -1   \n","3               wants to hang out with friends SOON!  1   \n","4  @dannycastillo We want to trade with someone w...  0   \n","\n","                                   processed_content  \n","0  tiffanylu know listenin bad habit earlier star...  \n","1              layin n bed headach ughhhhwaitin call  \n","2                        funer ceremonygloomi friday  \n","3                              want hang friend soon  \n","4  dannycastillo want trade someon houston ticket...  "],"text/html":["\n","  <div id=\"df-544b31e2-f0ab-445d-8242-c2845c26fc8d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>sentiment</th>\n","      <th>author</th>\n","      <th>content</th>\n","      <th>y</th>\n","      <th>processed_content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1956967341</td>\n","      <td>empty</td>\n","      <td>xoshayzers</td>\n","      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n","      <td>-1</td>\n","      <td>tiffanylu know listenin bad habit earlier star...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1956967666</td>\n","      <td>sadness</td>\n","      <td>wannamama</td>\n","      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n","      <td>-1</td>\n","      <td>layin n bed headach ughhhhwaitin call</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1956967696</td>\n","      <td>sadness</td>\n","      <td>coolfunky</td>\n","      <td>Funeral ceremony...gloomy friday...</td>\n","      <td>-1</td>\n","      <td>funer ceremonygloomi friday</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1956967789</td>\n","      <td>enthusiasm</td>\n","      <td>czareaquino</td>\n","      <td>wants to hang out with friends SOON!</td>\n","      <td>1</td>\n","      <td>want hang friend soon</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1956968416</td>\n","      <td>neutral</td>\n","      <td>xkilljoyx</td>\n","      <td>@dannycastillo We want to trade with someone w...</td>\n","      <td>0</td>\n","      <td>dannycastillo want trade someon houston ticket...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-544b31e2-f0ab-445d-8242-c2845c26fc8d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-544b31e2-f0ab-445d-8242-c2845c26fc8d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-544b31e2-f0ab-445d-8242-c2845c26fc8d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-f6c04b5f-fc2a-48eb-99d2-d74d54b9a627\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f6c04b5f-fc2a-48eb-99d2-d74d54b9a627')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-f6c04b5f-fc2a-48eb-99d2-d74d54b9a627 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"emotion_raw_csv","summary":"{\n  \"name\": \"emotion_raw_csv\",\n  \"rows\": 40000,\n  \"fields\": [\n    {\n      \"column\": \"tweet_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 118857896,\n        \"min\": 1693956175,\n        \"max\": 1966441171,\n        \"num_unique_values\": 40000,\n        \"samples\": [\n          1752414968,\n          1965295852,\n          1696219218\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          \"relief\",\n          \"happiness\",\n          \"empty\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33871,\n        \"samples\": [\n          \"AmberLovesNKOTB\",\n          \"HeartPanda\",\n          \"deucehartley\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 39827,\n        \"samples\": [\n          \"the sun is shinning! im off out!!\",\n          \"wow i must have been tired. i fell asleep @ exactly the start of the 10pm news &amp; now up as if its 8am.\",\n          \"@knobzie their loss\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": -1,\n        \"max\": 1,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1,\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processed_content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 39236,\n        \"samples\": [\n          \"interest watch snl ciara justin timberfak\",\n          \"hotnizz wah sayang\",\n          \"omj qot home parti im man tire goodniqht happi mother day take care amp god bless\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":131}],"source":["# Run this code to compare the original content with your processed text\n","# Displaying the first 5 rows of the updated DataFrame\n","emotion_raw_csv.head(5)"]},{"cell_type":"markdown","metadata":{"id":"4UNwgDGEGK05"},"source":["### c) Next, complete the code below to split the data into a training set and a test set with the split rate 80% : 20%."]},{"cell_type":"code","execution_count":132,"metadata":{"id":"Y7zpHR14MS3d","executionInfo":{"status":"ok","timestamp":1719461569874,"user_tz":240,"elapsed":2,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}}},"outputs":[],"source":["# Complete the last line of code to split the data into training and testing sets.\n","\n","# Define 'X' variable\n","X = emotion_raw_csv['processed_content']\n","# Recall that 'y' variable is already defined as y = emotion_raw_csv['y']\n","\n","# Split the data into training and testing set:\n","X_train, X_test, y_train, y_test =  train_test_split(X, y,  test_size=0.20, random_state=42)\n"]},{"cell_type":"markdown","metadata":{"id":"CnaQv8crJ4jN"},"source":["## Q3: **Vectorization**"]},{"cell_type":"markdown","metadata":{"id":"k95xNE0eGFaQ"},"source":["### a) Pick one vectorizer from the follows:\n","*   Count Vectorizer (BoW)\n","*   TF-IDF\n","*   BoW with Stopword Removal\n","*   BoW with n-gram and Stopword Removal"]},{"cell_type":"code","execution_count":133,"metadata":{"id":"4F-qygifRRAY","executionInfo":{"status":"ok","timestamp":1719461569874,"user_tz":240,"elapsed":2,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}}},"outputs":[],"source":["# Example Vectorizers using sklearn library\n","# Pick one of these, and uncomment it\n","\n","# Count Vectorizer (BoW)\n","#vectorizer = text.CountVectorizer()\n","\n","# TF-IDF Vectorizer\n","#vectorizer = text.TfidfVectorizer()\n","\n","# BOW with stopword removal\n","#vectorizer = text.CountVectorizer(stop_words=stopwords)\n","\n","# 2-grams with stopword removal\n","vectorizer = text.CountVectorizer(ngram_range=(1,2),stop_words=stopwords)\n"]},{"cell_type":"markdown","metadata":{"id":"QUQcfV8tNqq1"},"source":["### b) Fit the vectorizer to the training set and transform the test set.\n","\n","We provide the codes for demonstrating how to fit / transform the vectorizer. You can just run it without any editings needed."]},{"cell_type":"code","execution_count":134,"metadata":{"id":"Pt3U5IjSRufE","executionInfo":{"status":"ok","timestamp":1719461572077,"user_tz":240,"elapsed":2204,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}}},"outputs":[],"source":["# You fit and transform your training text and just transform the test text\n","# fit_transform builds the dictionary and transforms the text\n","# transform function uses the existing dictionary to transform text\n","# You must use the same dictionary or else the model shape will not match\n","X_train_vectors = vectorizer.fit_transform(X_train)\n","X_test_vectors = vectorizer.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"OVMhs7lRROEO"},"source":["### c) As we increase the `n` in the n-gram vectorizer, how does the size of the feature names change?"]},{"cell_type":"markdown","metadata":{"id":"oqo3T1nyNxbB"},"source":["**Answer:** If n=2, in bigram mode, For instance, the phrase \"the quick brown fox\" would take considerations as combinations like \"the quick\", \"quick brown\", and \"brown fox\".<br>\n","In trigram mode (n=3), it would include \"the quick brown\", \"quick brown fox\", etc.<br>\n","There will be increase in feature sizes, and feature matrix becomes more sparse.The dimensionality of the feature matrix increases. This could lead to overfitting. Removing stop words reduces the number of potential n-grams.<br> This also means Increased Computational Load, memory usage, it can improve model performance with increased model complexity."]},{"cell_type":"markdown","metadata":{"id":"zUv7MXTuQ8kl"},"source":["### d) Describe how does 2-gram differ from BoW and TF-IDF."]},{"cell_type":"markdown","metadata":{"id":"bgIaqGlIRATp"},"source":["**Answer:** 2-Gram(bigram), is pairs of words. It can capture context and sequence of the words.<br>\n","Bag of Words (BoW), cares about occurrence of words. It measures the presence of defined words. By default, it uses 1-Gram.<br>\n","TF-IDF (Term Frequency-Inverse Document Frequency), It calculates a term’s frequency (TF) and its inverse document frequency (IDF). Each word or term is given a weight in the document. TF-IDF considers not just the occurrence of a word in a single document but in the entire corpus, across documents, TF-IDF can highlight which words are truly distinctive in a document."]},{"cell_type":"markdown","metadata":{"id":"3N0EBj1eF5lc"},"source":["### e) What is the # of feature names stored in the vectorizor? (To answer this question, you will first need to write code and run it)"]},{"cell_type":"code","execution_count":135,"metadata":{"id":"ceEgy9GoSHLD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719461572392,"user_tz":240,"elapsed":317,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}},"outputId":"5d75608d-c28e-49d4-9668-a2814c3598f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of features (bigrams) in the vectorizer: 187730\n"]}],"source":["num_features = len(vectorizer.get_feature_names_out())\n","\n","print(f\"Number of features (bigrams) in the vectorizer: {num_features}\")"]},{"cell_type":"markdown","metadata":{"id":"m8umkTfDY4u7"},"source":["**Answer:** for 2-gram, the answer is 187730. (if I change it to 3-gram, the number will be 354763)"]},{"cell_type":"markdown","metadata":{"id":"bja3OXDhTDgK"},"source":["## Q4: **Naive-Bayes Model**\n","Use Naive-Bayes (MultinomialNB) within sklearn to fit your model and test your results.\n"]},{"cell_type":"markdown","metadata":{"id":"7HZLJVRhTDgM"},"source":["### a) Build your Naive-Bayes model on the training set, and generate predictions on the test set.\n","We provide the codes for Naive-Bayes model initialization, training and prediction. You can just run it without any editings needed."]},{"cell_type":"code","execution_count":136,"metadata":{"id":"O8Vd0IfqUWla","executionInfo":{"status":"ok","timestamp":1719461572393,"user_tz":240,"elapsed":7,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}}},"outputs":[],"source":["## Fit the Naive-Bayes Model and Form a prediction\n","# Your variables for X and y may be different.\n","# You will need to ensure you are passing the vectorized X in and the Y labels\n","\n","# Initialize a multinomial Naive Bayes model\n","model = naive_bayes.MultinomialNB()\n","\n","# Fit/train the model using the training data\n","model.fit(X_train_vectors, y_train)\n","\n","# Use the model to make prediction using the testing data\n","y_pred = model.predict(X_test_vectors)"]},{"cell_type":"markdown","metadata":{"id":"uf8WoF9OSbSW"},"source":["### b)  Evaluate your Naive-Bayes model performance with Precision, Recall, and F1 Score.\n","We provide the codes for model performance evaluation with Precision, Recall, and F1 Score.\n","\n","You can just run it without any editings needed."]},{"cell_type":"code","execution_count":137,"metadata":{"id":"_-1oRf7uUWlc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719461572393,"user_tz":240,"elapsed":6,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}},"outputId":"b17d604a-9c44-4042-b382-3f1cda0fb90f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Naive-Bayes Performance by Class (entries are ordered as negative, neutral and positive)\n","Precision: [0.53455571 0.49425287 0.59092599]\n","Recall: [0.83114035 0.07944573 0.60121075]\n","F1 Score: [0.65064378 0.13688818 0.59602401]\n"]}],"source":["# Get our performance metrics, precision, recall, F1\n","\n","precision_class = precision_score(y_test, y_pred, average=None, zero_division=0.0)\n","recall_class = recall_score(y_test, y_pred, average=None, zero_division=0.0)\n","f1_class = f1_score(y_test, y_pred, average=None, zero_division=0.0)\n","\n","print(\"{:=^50s}\".format(\"Naive-Bayes Performance by Class (entries are ordered as negative, neutral and positive)\"))\n","\n","# Output results by class\n","print(\"Precision:\", precision_class)\n","print(\"Recall:\", recall_class)\n","print(\"F1 Score:\", f1_class)"]},{"cell_type":"markdown","metadata":{"id":"XR6SLGqhZx_y"},"source":["Next, compute model performance evaluation with weighted Precision, Recall, and F1 Score.\n","\n","Hint: use the code above, but change `average=None` to `average='weighted'`"]},{"cell_type":"code","execution_count":138,"metadata":{"id":"FYGULAyJZ39-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719461572393,"user_tz":240,"elapsed":4,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}},"outputId":"2e7876af-aff3-4398-fa2d-b303b0f64dd7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Naive-Bayes Performance by Class (entries are ordered as negative, neutral and positive). weighted\n","Precision: 0.5422720886340335\n","Recall: 0.55175\n","F1 Score: 0.4935636620421808\n"]}],"source":["# Get our performance metrics, precision, recall, F1\n","\n","precision_class = precision_score(y_test, y_pred, average='weighted', zero_division=0.0)\n","recall_class = recall_score(y_test, y_pred, average='weighted', zero_division=0.0)\n","f1_class = f1_score(y_test, y_pred, average='weighted', zero_division=0.0)\n","\n","print(\"{:=^50s}\".format(\"Naive-Bayes Performance by Class (entries are ordered as negative, neutral and positive). weighted\"))\n","\n","# Output results by class\n","print(\"Precision:\", precision_class)\n","print(\"Recall:\", recall_class)\n","print(\"F1 Score:\", f1_class)"]},{"cell_type":"markdown","metadata":{"id":"HBS-lCkCTDgQ"},"source":["### c) Explain the meaning of precision and recall, and interpret the model performance with the above results."]},{"cell_type":"markdown","metadata":{"id":"GnSNa-F_YHvK"},"source":["**Answer:** Precision: Precision = True Positives / (True Positives + False Positives). It measures correctly predicted positive observations to the total predicted positives. <br>\n","Recall: Recall = True Positives / (True Positives + False Negatives). It measures correctly predicted positive observations to all observations in the actual class.<br>\n","The F1 Score: F1 Score = 2 * (Precision * Recall) / (Precision + Recall). It is the weighted average of Precision and Recall. It is particularly useful when the class distribution is uneven. The F1 Score takes both false positives and false negatives into account, providing a more holistic view of the model's performance. <br>\n","For weighted results: Precision 0.544 (54.4%). This is low, indicating that the model may be generating a considerable number of false positives.<br>\n","Recall 0.54875. This suggests moderate sensitivity, meaning that the model misses a good portion of positive cases (around 45.1%).<br>\n","The F1 Score of 48.7% is relatively low, suggesting that the model does not perform exceptionally well on either front, either leaving out several positive cases or incorrectly labeling negatives as positives.<br>\n","for the average none, the number is for positive, neutral and negative specifically."]},{"cell_type":"markdown","metadata":{"id":"FMLCMqjX-UIE"},"source":["## Q5 **Multi-Class Logistic Regression**\n","Next, let us train a logistic regression model to perform the same classification task. We will then evaluate the model using the same testing data."]},{"cell_type":"markdown","metadata":{"id":"kZF7FppjfbfZ"},"source":["### a) Build your mutinomial logistic regression model on the training set, and generate predictions on the test set.\n","Hint: You may check the Q4 a) for model initialization, training and prediction. Also, do not worry about enabling multinomial LR, the LogisticRegression algorithm from sklearn chooses the multinomial LR automatically once it recognize it has more than two classes."]},{"cell_type":"code","execution_count":139,"metadata":{"id":"-TIzG1Ctdj4k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719461608303,"user_tz":240,"elapsed":35913,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}},"outputId":"38fcf158-ea6e-4d43-f43e-c5278f0ecb93"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}],"source":["\n","# Building the multinomial logistic regression model\n","log_reg_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)\n","log_reg_model.fit(X_train_vectors, y_train)\n","\n","# Predicting the labels for the test set\n","y_pred = log_reg_model.predict(X_test_vectors)"]},{"cell_type":"markdown","metadata":{"id":"cLv71H0jxU3D"},"source":["### b)  Evaluate your LR model performance with Precision, Recall, and F1 Score.\n","\n","Hint: Again, you may check some of the codes provided in Q4."]},{"cell_type":"code","execution_count":140,"metadata":{"id":"YZ0_T8csxYN2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719461608303,"user_tz":240,"elapsed":13,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}},"outputId":"defd464d-3b2b-413e-c2c6-49edb1ec99b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression Performance by Class (entries are ordered as negative, neutral and positive)\n","Precision: [0.61684149 0.43657817 0.61878453]\n","Recall: [0.66322055 0.41016166 0.59326523]\n","F1 Score: [0.63919082 0.42295785 0.60575623]\n"]}],"source":["# Get our performance metrics, precision, recall, F1\n","\n","precision_class = precision_score(y_test, y_pred, average=None, zero_division=0.0)\n","recall_class = recall_score(y_test, y_pred, average=None, zero_division=0.0)\n","f1_class = f1_score(y_test, y_pred, average=None, zero_division=0.0)\n","\n","print(\"{:=^50s}\".format(\"Logistic Regression Performance by Class (entries are ordered as negative, neutral and positive)\"))\n","\n","# Output results by class\n","print(\"Precision:\", precision_class)\n","print(\"Recall:\", recall_class)\n","print(\"F1 Score:\", f1_class)"]},{"cell_type":"markdown","metadata":{"id":"S4LGXD2Rg9vA"},"source":["### c)  Evaluate your Logisitc Regression model performance with weighted Precision, Recall, and F1 Score."]},{"cell_type":"code","execution_count":141,"metadata":{"id":"JQbaUDdglcQo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719461608303,"user_tz":240,"elapsed":12,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}},"outputId":"7099ccb8-d49f-47e0-e47b-95b8092c876e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression Performance by Class (entries are ordered as negative, neutral and positive). Weighted\n","Precision: [0.61684149 0.43657817 0.61878453]\n","Recall: 0.571625\n","F1 Score: 0.5696268193676524\n"]}],"source":["recision_class = precision_score(y_test, y_pred, average='weighted', zero_division=0.0)\n","recall_class = recall_score(y_test, y_pred, average='weighted', zero_division=0.0)\n","f1_class = f1_score(y_test, y_pred, average='weighted', zero_division=0.0)\n","\n","print(\"{:=^50s}\".format(\"Logistic Regression Performance by Class (entries are ordered as negative, neutral and positive). Weighted\"))\n","\n","# Output results by class\n","print(\"Precision:\", precision_class)\n","print(\"Recall:\", recall_class)\n","print(\"F1 Score:\", f1_class)"]},{"cell_type":"markdown","metadata":{"id":"2KZzZRQFwbk1"},"source":["### d) Briefly discuss whether you prefer multinomial Naive Bayes or Logistic Regression for this classification task, and why."]},{"cell_type":"markdown","metadata":{"id":"ace5YWlWw5ZG"},"source":["**Answer:** I will choose LR.<br>NB is less precise compared to LR, lower in recall and slightly lower in F-1 score (better balance). <br>\n","Logistic Regression is more robust to the independence assumption since it does not assume that the features are conditionally independent given the class, unlike Naive Bayes.<br>\n","LR works better with larger dataset, but is also more computational intensive."]},{"cell_type":"markdown","metadata":{"id":"jniEu7VgK72S"},"source":["## Q6: **TextBlob**\n","In this quesiton, we will apply TextBlob from NLTK on the dataset for sentiment analysis. Recall that TextBlob returns a sentiment score between -1.0 (most negative) to +1.0 (most positive)."]},{"cell_type":"markdown","metadata":{"id":"rqSy5-qcatoM"},"source":["\n","### a) Run the code below that computes and stores the textblob scores for each (processed) tweet in the testing set."]},{"cell_type":"code","execution_count":142,"metadata":{"id":"_ETMq27mWWaN","executionInfo":{"status":"ok","timestamp":1719461609986,"user_tz":240,"elapsed":1694,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}}},"outputs":[],"source":["y_pred_textblob_score = X_test.apply(lambda x: TextBlob(str(x)).sentiment[0])"]},{"cell_type":"markdown","metadata":{"id":"of31-RsjazZM"},"source":["### b) We are going to use TextBlob to classify the tweets as follows: we first select the positive cutoff to be +0.33, and the negative cutoff to be -0.33.Then, we classify all tweets with scores > 0.33 as +1 (positive), all tweets with scores < -0.33 as -1 (negative), and the rest of the tweets with -0.33 <= scores <= 0.33 as 0 (neutral).\n","\n","Based on the selected cutoffs, classify tweets as -1, 0, 1 using the Textblob sentiment scores."]},{"cell_type":"code","execution_count":143,"metadata":{"id":"YFayXC1ra6Me","collapsed":true,"executionInfo":{"status":"ok","timestamp":1719461609986,"user_tz":240,"elapsed":3,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}}},"outputs":[],"source":["# Your code for the question above.\n","\n","# Define cutoffs\n","positive_cutoff = 0.33\n","negative_cutoff = -0.33\n","\n","# Hint: you may want to use the code below but you need to define the right cutoffs\n","y_pred_textblob = y_pred_textblob_score.apply(lambda x: 1 if x > positive_cutoff else (-1 if x < negative_cutoff else 0))\n"]},{"cell_type":"markdown","metadata":{"id":"-U2M7JHQbMe7"},"source":["### c) Evaluate your Textblob model performance with weighted Precision, Recall, and F1 Score.\n","Hint: You may follow the codes provided for Q4 b)."]},{"cell_type":"code","execution_count":144,"metadata":{"id":"_gN2EpKibODK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719461609986,"user_tz":240,"elapsed":2,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}},"outputId":"ec3b41ae-317a-4c62-c252-cbdaf9c85d6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["TextBlob Performance by Class (entries are ordered as negative, neutral and positive)\n","Precision: 0.5565483975567815\n","Recall: 0.3835\n","F1 Score: 0.34671952329699124\n"]}],"source":["# Your code for the question above.\n","\n","# Calculate precision, recall, and F1 score with 'weighted' average\n","precision_class = precision_score(y_test, y_pred_textblob, average='weighted', zero_division=0)\n","recall_class = recall_score(y_test, y_pred_textblob, average='weighted', zero_division=0)\n","f1_class = f1_score(y_test, y_pred_textblob, average='weighted', zero_division=0)\n","\n","print(\"{:=^50s}\".format(\"TextBlob Performance by Class (entries are ordered as negative, neutral and positive)\"))\n","# Print the performance metrics\n","print(\"Precision:\", precision_class)\n","print(\"Recall:\", recall_class)\n","print(\"F1 Score:\", f1_class)"]},{"cell_type":"markdown","metadata":{"id":"Tr70jzCrbid3"},"source":["### d) Would a different set of cutoffs improve Textblob's performance? If so, how would you find such a set without using the testing data set? (The 5 bonus points are reserved for coding up an implementation to optimize the cutoffs)"]},{"cell_type":"markdown","metadata":{"id":"Dju1Hmrpbjq-"},"source":["Please enter your answer to the question above here:"]},{"cell_type":"code","execution_count":145,"metadata":{"id":"CjfRJkwy1Z2R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719461619641,"user_tz":240,"elapsed":9657,"user":{"displayName":"Qi Tan","userId":"00069661742172985235"}},"outputId":"53143120-858b-4440-cc65-b9984da81f21"},"outputs":[{"output_type":"stream","name":"stdout","text":["Best Cutoffs: (0.050000000000000044, -0.04999999999999999) with F1 Score: 0.4235336093844955\n"]}],"source":["import numpy as np\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import f1_score\n","from textblob import TextBlob\n","\n","# Define a function to calculate sentiment scores using TextBlob\n","def get_sentiment(text):\n","    return TextBlob(text).sentiment.polarity\n","\n","# Define a function to classify sentiments based on cutoffs\n","def classify_with_cutoffs(sentiments, positive_cutoff, negative_cutoff):\n","    return np.array([1 if score > positive_cutoff else -1 if score < negative_cutoff else 0 for score in sentiments])\n","\n","# Prepare sentiment scores for training data\n","X_train_sentiments = np.array([get_sentiment(text) for text in X_train])\n","\n","# Setting up K-Fold cross-validation\n","kf = KFold(n_splits=5)  # 5-fold cross-validation\n","best_score = 0\n","best_cutoffs = (0, 0)\n","cutoffs = np.linspace(-0.5, 0.5, 21)  # Generates 21 points from -0.5 to 0.5\n","\n","for pos_cutoff in cutoffs[cutoffs > 0]:\n","    for neg_cutoff in cutoffs[cutoffs < 0]:\n","        scores = []\n","        for train_index, val_index in kf.split(X_train_sentiments):\n","            # Get training and validation subsets for this fold\n","            # Use .iloc to access elements by position since the index might not be a simple range\n","            X_train_fold, y_train_fold = X_train_sentiments[train_index], y_train.iloc[train_index]\n","            X_val_fold, y_val_fold = X_train_sentiments[val_index], y_train.iloc[val_index]\n","\n","            # Classify sentiments in the validation fold\n","            y_val_pred = classify_with_cutoffs(X_val_fold, pos_cutoff, neg_cutoff)\n","            # Calculate F1 score for this fold and cutoffs\n","            score = f1_score(y_val_fold, y_val_pred, average='weighted', zero_division=0)\n","            scores.append(score)\n","\n","        # Average score across all folds\n","        avg_score = np.mean(scores)\n","        if avg_score > best_score:\n","            best_score = avg_score\n","            best_cutoffs = (pos_cutoff, neg_cutoff)\n","\n","print(\"Best Cutoffs:\", best_cutoffs, \"with F1 Score:\", best_score)\n","#Best Cutoffs: (0.050000000000000044, -0.04999999999999999) with F1 Score: 0.4235336093844955"]},{"cell_type":"markdown","metadata":{"id":"M0IFu26biX6V"},"source":["\n","\n","---\n","## End of Assignment\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1itJxPoi8WGa-33BcjtfJMVHew7fX6-63","timestamp":1719461782511}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}