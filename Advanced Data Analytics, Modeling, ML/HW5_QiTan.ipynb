{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwLIDHtkB-Pg"
      },
      "source": [
        "# QM 701: Advanced Data Analytics and Applications\n",
        "# Homework 5\n",
        "---\n",
        "## Objective\n",
        "\n",
        "The goal of this homework is get you some hands-on experience finetuning BERT models. We will be working with the dataset containing financial news articles before 2013 on companies in Finland stock market.\n",
        "\n",
        "## Tasks\n",
        "This homework includes the following 5 questions.\n",
        " - **Q1**: Inspecting and Splitting Dataset. (15 points)\n",
        " - **Q2**: Establishing Benchmarks (20 points)\n",
        " - **Q3**: Classification with the Pre-trained BERT Model (15 points)\n",
        " - **Q4**: Finetuning the BERT Model (50 points)\n",
        " - **Bonus** Sentiment Analysis with FinBERT (10 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "SHwBSBBds4lU"
      },
      "outputs": [],
      "source": [
        "# General dataframe imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "# sklearn imports\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import naive_bayes\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import text\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOLeyjxeF8F1"
      },
      "source": [
        "## Data Setup\n",
        "We will use the same financial news dataset as in HW 4. This dataset contains 4846 sentences extracted from financial news articles about companies listed on the Finnish stock market. Each sentence is classified into one of three categories: **\"negative,\" \"neutral,\" or \"positive.\"**\n",
        "\n",
        "To speed up model training, we will sample 50% of these sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3WSrQuKjK9a"
      },
      "source": [
        "### Download Dataset\n",
        "We download the dataset and store it in a pandas dataframe `news_df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUQ_kvWyvpyI",
        "outputId": "bdba77e5-fc7d-46c4-999c-45e03ff84f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-09 20:13:34--  https://duke.box.com/shared/static/anno25ihglqxif8jcjoo4f7p7vd2ajt3\n",
            "Resolving duke.box.com (duke.box.com)... 74.112.186.157\n",
            "Connecting to duke.box.com (duke.box.com)|74.112.186.157|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/anno25ihglqxif8jcjoo4f7p7vd2ajt3 [following]\n",
            "--2024-08-09 20:13:35--  https://duke.box.com/public/static/anno25ihglqxif8jcjoo4f7p7vd2ajt3\n",
            "Reusing existing connection to duke.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://duke.app.box.com/public/static/anno25ihglqxif8jcjoo4f7p7vd2ajt3 [following]\n",
            "--2024-08-09 20:13:35--  https://duke.app.box.com/public/static/anno25ihglqxif8jcjoo4f7p7vd2ajt3\n",
            "Resolving duke.app.box.com (duke.app.box.com)... 74.112.186.157\n",
            "Connecting to duke.app.box.com (duke.app.box.com)|74.112.186.157|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!f6ZjXMnoAbWoFLM8DVKAIche6csqRsuC59IhFhocmE1_uYTvYtb2kS0bH4k9tsSD6gdxCwjFiOoS6Sf5IbjHxS-UI69HKhShWL8f8_LwzT83Tj-wufo0uby2t2um0TMqPKUQsiLiAoHpU61D0Wj_NoBr1scZie70-IoBotELHFAMyU8ZGtm6M3Br_SoQTRsMppmdefLwZRqA8O6v8ehpRLbCKdXf30pEaalMe-pmz0K0KeP4H1jP7rx-6OEdAC1J2Mdpj8gyNYQWRrJeddf9iP2DDkNNTp5SCmS-N7giFyUj_EROmN1vyJ30Rpgl07KUzH407_0E8bMT1It1htRei-aI2_I7NXzhquFLLk2jqKPM88-t_0EKnyeJiAoM6uBzdkwUvh6Jl6oY35uJ05BvNxGofjG5tROIgRqAVxapqUsSK_0uPmKKkPze2fzz5PpEFqmFPuxudxk6vNc8tVk9yZ0jkRi--Ogg6w1ikt726Sqe1CI4LersdF5UuhorV13ifqWFhGyxV2h27O1NHUtm1xPemcEnbLFB3F9EX5wAUJo02HCaScy6CvpMfQ02z7wxNpd5_DYaOQsY2uKPUGkoa5W8QvTLG1E4yMQQUFhtWSYLqxanG-hTA6ys4MAkKAwEFug4iOlu8EU2lZhJd63T5vwY2zHFw46FXar5mlTwErC0Ib_erov1XvaDJ0Ben03CsN8nm5eQYW0X6U0ma8IjIC1dtJtS7gST_Jc1w-WXulwguKMXyiXP417Li6oZH6v_YBVBtFi5a7Q9MWjVN_688-3roVr_1FVO_ugPUCgbHvSPC2U8yF7OJflCqql8xhjexi3cBFFstTzlszxmexYgDZWyBjiZlii6mpfkgtdMQgcT4vrlTL4IkPEOsvYhpJuNgsSw55MCIya5xerEIsegPigQQ8fg24LgwXPHbhy98f_pfECGUB0pQ3fgI4fy5cA4vM9VRNqQuI13pX9ZVI4l5xntxmkbTeFlYhSgmXvWb3jgl6z4rHvN_YMDWixNBrUdHcYGaMPkEk9HVQ1LQsbD6cFLDqfsRjK98ioL1NhFOZiXVUHZOPjEPlrCM7do7d-k0gWE14VLd49j7CGljlN-9c81j1tUCjg9H6PKPX_bAgyhr-53fjeqz6SWnOetDKUvt1zVp2yf4UshP9j4IDs3kivDbq2-6J8K09DrA-leQnxsT4sni2Rauz4U_2KvrbEE2JdbM3tO-BgTyg22noc4B_QX6WNUuV5OmaSJekAx8Ogy4HtpbEHfDu6XHuqcDiByCSqX-7qi35WyeEmWbVWZicX7-18mY8bLb6K2Hb5qBYDe_3DDOHDEBW8q9Wk9uxUBnJO5O85knVzxaPWyMJAIlsclYA../download [following]\n",
            "--2024-08-09 20:13:35--  https://public.boxcloud.com/d/1/b1!f6ZjXMnoAbWoFLM8DVKAIche6csqRsuC59IhFhocmE1_uYTvYtb2kS0bH4k9tsSD6gdxCwjFiOoS6Sf5IbjHxS-UI69HKhShWL8f8_LwzT83Tj-wufo0uby2t2um0TMqPKUQsiLiAoHpU61D0Wj_NoBr1scZie70-IoBotELHFAMyU8ZGtm6M3Br_SoQTRsMppmdefLwZRqA8O6v8ehpRLbCKdXf30pEaalMe-pmz0K0KeP4H1jP7rx-6OEdAC1J2Mdpj8gyNYQWRrJeddf9iP2DDkNNTp5SCmS-N7giFyUj_EROmN1vyJ30Rpgl07KUzH407_0E8bMT1It1htRei-aI2_I7NXzhquFLLk2jqKPM88-t_0EKnyeJiAoM6uBzdkwUvh6Jl6oY35uJ05BvNxGofjG5tROIgRqAVxapqUsSK_0uPmKKkPze2fzz5PpEFqmFPuxudxk6vNc8tVk9yZ0jkRi--Ogg6w1ikt726Sqe1CI4LersdF5UuhorV13ifqWFhGyxV2h27O1NHUtm1xPemcEnbLFB3F9EX5wAUJo02HCaScy6CvpMfQ02z7wxNpd5_DYaOQsY2uKPUGkoa5W8QvTLG1E4yMQQUFhtWSYLqxanG-hTA6ys4MAkKAwEFug4iOlu8EU2lZhJd63T5vwY2zHFw46FXar5mlTwErC0Ib_erov1XvaDJ0Ben03CsN8nm5eQYW0X6U0ma8IjIC1dtJtS7gST_Jc1w-WXulwguKMXyiXP417Li6oZH6v_YBVBtFi5a7Q9MWjVN_688-3roVr_1FVO_ugPUCgbHvSPC2U8yF7OJflCqql8xhjexi3cBFFstTzlszxmexYgDZWyBjiZlii6mpfkgtdMQgcT4vrlTL4IkPEOsvYhpJuNgsSw55MCIya5xerEIsegPigQQ8fg24LgwXPHbhy98f_pfECGUB0pQ3fgI4fy5cA4vM9VRNqQuI13pX9ZVI4l5xntxmkbTeFlYhSgmXvWb3jgl6z4rHvN_YMDWixNBrUdHcYGaMPkEk9HVQ1LQsbD6cFLDqfsRjK98ioL1NhFOZiXVUHZOPjEPlrCM7do7d-k0gWE14VLd49j7CGljlN-9c81j1tUCjg9H6PKPX_bAgyhr-53fjeqz6SWnOetDKUvt1zVp2yf4UshP9j4IDs3kivDbq2-6J8K09DrA-leQnxsT4sni2Rauz4U_2KvrbEE2JdbM3tO-BgTyg22noc4B_QX6WNUuV5OmaSJekAx8Ogy4HtpbEHfDu6XHuqcDiByCSqX-7qi35WyeEmWbVWZicX7-18mY8bLb6K2Hb5qBYDe_3DDOHDEBW8q9Wk9uxUBnJO5O85knVzxaPWyMJAIlsclYA../download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 74.112.186.128\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|74.112.186.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 672006 (656K) [text/csv]\n",
            "Saving to: ‘all-data.csv’\n",
            "\n",
            "all-data.csv        100%[===================>] 656.26K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-08-09 20:13:35 (62.6 MB/s) - ‘all-data.csv’ saved [672006/672006]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download the dataset\n",
        "!wget https://duke.box.com/shared/static/anno25ihglqxif8jcjoo4f7p7vd2ajt3 -O all-data.csv\n",
        "# store as news_df\n",
        "news_df = pd.read_csv(\"all-data.csv\", names=[\"sentiment\", \"content\"], encoding=\"latin-1\")\n",
        "# sample 50%\n",
        "news_df = news_df.sample(frac=0.5, random_state=41)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o3tWHmejEhC"
      },
      "source": [
        "## Q1: Inspecting and Splitting the Dataset\n",
        "\n",
        "In this question, you will first take a quick look at the dataset. Then, you will split the dataset into training, validation, and testing sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxVJQNXxFVDG"
      },
      "source": [
        "### a) Sentiment Distribution\n",
        "\n",
        "We will plot the distribution of the sentiments in this dataset. You can run the code below without any edits.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "8sGr14_5fH2-",
        "outputId": "ec7756ed-27ec-4b35-de5b-f3dd23e5b052"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     sentiment  \\\n",
              "826   positive   \n",
              "137   positive   \n",
              "1271   neutral   \n",
              "265   positive   \n",
              "2214   neutral   \n",
              "\n",
              "                                                                                                                                                  content  \n",
              "826   BasWare 's CEO Ilkka Sihvo comments in conjunction with the Interim Report : `` As a whole , BasWare succeeded well in the second quarter of 2007 .  \n",
              "137                   `` The purchase of the operations is part of YIT 's strategy to expand its offering of building system services geographically . ''  \n",
              "1271                       Jon Risfelt is 49 years old holds a Master of Science in Chemical Engineering from the Swedish Royal Institute of Technology .  \n",
              "265                                                                Pretax profit rose to EUR 0.6 mn from EUR 0.4 mn in the corresponding period in 2005 .  \n",
              "2214                                                                     The company 's operating profit for 2006 stood at 39.6 mln euro ( $ 57.9 mln ) .  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0764cb37-4171-4b7a-84b6-139d5f7d3f2b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>826</th>\n",
              "      <td>positive</td>\n",
              "      <td>BasWare 's CEO Ilkka Sihvo comments in conjunction with the Interim Report : `` As a whole , BasWare succeeded well in the second quarter of 2007 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>positive</td>\n",
              "      <td>`` The purchase of the operations is part of YIT 's strategy to expand its offering of building system services geographically . ''</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1271</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Jon Risfelt is 49 years old holds a Master of Science in Chemical Engineering from the Swedish Royal Institute of Technology .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265</th>\n",
              "      <td>positive</td>\n",
              "      <td>Pretax profit rose to EUR 0.6 mn from EUR 0.4 mn in the corresponding period in 2005 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2214</th>\n",
              "      <td>neutral</td>\n",
              "      <td>The company 's operating profit for 2006 stood at 39.6 mln euro ( $ 57.9 mln ) .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0764cb37-4171-4b7a-84b6-139d5f7d3f2b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0764cb37-4171-4b7a-84b6-139d5f7d3f2b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0764cb37-4171-4b7a-84b6-139d5f7d3f2b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a4d54676-5def-478c-b301-99c6c4b61d53\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a4d54676-5def-478c-b301-99c6c4b61d53')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a4d54676-5def-478c-b301-99c6c4b61d53 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "news_df",
              "summary": "{\n  \"name\": \"news_df\",\n  \"rows\": 2423,\n  \"fields\": [\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"positive\",\n          \"neutral\",\n          \"negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2419,\n        \"samples\": [\n          \"After the split , the company would have 26,885,540 Series A shares and 9,540,000 Series K shares .\",\n          \"Overall , 50 percent of respondents were confident about their security when banking online .\",\n          \"The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# Display the first few rows in news_df\n",
        "news_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "v9_dH-yOGKwD",
        "outputId": "4f1f038f-b280-4cf2-8807-513fe1d72a07"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Compute the data distribution by sentiments\n",
        "data_counts = news_df[\"sentiment\"].value_counts()\n",
        "\n",
        "# Plot the results\n",
        "ax = sns.barplot(x=data_counts.index, y=data_counts.values, hue=data_counts.index, palette=\"husl\", legend=False, width=0.5)\n",
        "[ax.bar_label(bars) for bars in ax.containers]\n",
        "\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.title(\"Distribution of Sentiments\", fontsize=14)\n",
        "plt.xlabel(\"Sentiment\", fontsize=14)\n",
        "plt.ylabel('Count', fontsize=14)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKZXCSZXkxeM"
      },
      "source": [
        "### b) Using Accuracy as a Performance Measure\n",
        "\n",
        "As you can see from part (a), the sentiments are unevenly distributed, with more neutral sentences than positive and negative ones. Given this distribution of sentiment, discuss whether the **accuracy** score would be a good performance measure for evaluating a classification model built for this dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTdpq7p3fH2_"
      },
      "source": [
        "Your answer to the question above:\n",
        "The sentiments in the dataset are unevenly distributed, with more neutral sentences than positive and negative ones.\n",
        "\n",
        "The accuracy score alone is not be the best performance measure for evaluating the classification model. Accuracy is calculated as the number of correctly classified instances divided by the total number of instances. However, in imbalanced datasets like this, accuracy can be misleading.  \n",
        "   \n",
        "In the dataset, if the majority class is neutral sentences and the model simply predicts every instance as neutral, it may achieve a high accuracy score. However, this doesn't necessarily mean that the model is performing well in terms of correctly identifying positive or negative sentiments.  \n",
        "\n",
        "If we use other measures such as: Precision, Recall, F1 Score or Area Under the ROC Curve (AUC-ROC): AUC-ROC measures the model's ability to distinguish between positive and negative instances. It considers the trade-off between true positive rate and false positive rate and provides a single score that represents the overall performance.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cugm-58Ljwgj"
      },
      "source": [
        "### c) Inspecting the Dataset\n",
        "\n",
        "Please randomly display one sentence for each of the positive, negative, and neutral sentiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "a7c6HXMmfH2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c9d244-1083-4948-dcb5-9f5ecc1713de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive sentiment:\n",
            "Svyturys-Utenos Alus , which is controlled by the Nordic group Baltic Beverages Holding ( BBH ) , posted a 4.7-per-cent growth in beer sales for January-May to 46.22 million litres .\n",
            "\n",
            "Negative sentiment:\n",
            "The production is to be liquidated before June 2009 and 325 employees loose their jobs .\n",
            "\n",
            "Neutral sentiment:\n",
            "The dividend will come on top of the 0.45 eur on A shares and 0.43 on K shares it has already paid on last year 's accounts .\n"
          ]
        }
      ],
      "source": [
        "# Your code for the question above.\n",
        "\n",
        "# Filter sentences for each sentiment\n",
        "positive_sentences = news_df[news_df['sentiment'] == 'positive']['content']\n",
        "negative_sentences = news_df[news_df['sentiment'] == 'negative']['content']\n",
        "neutral_sentences = news_df[news_df['sentiment'] == 'neutral']['content']\n",
        "\n",
        "# Randomly select one sentence from each sentiment\n",
        "positive_sentence = positive_sentences.sample(n=1).values[0]\n",
        "negative_sentence = negative_sentences.sample(n=1).values[0]\n",
        "neutral_sentence = neutral_sentences.sample(n=1).values[0]\n",
        "\n",
        "# Display the randomly selected sentences\n",
        "print(\"Positive sentiment:\")\n",
        "print(positive_sentence)\n",
        "print()\n",
        "\n",
        "print(\"Negative sentiment:\")\n",
        "print(negative_sentence)\n",
        "print()\n",
        "\n",
        "print(\"Neutral sentiment:\")\n",
        "print(neutral_sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSfsdfbBeR0W"
      },
      "source": [
        "### d) Train-test Split\n",
        "\n",
        "Split the dataset such that training (`X_train` and `y_train`) contain 60% of the data, validation (`X_val` and `y_val`) contain 20% of the data, and test (`X_test` and `y_test`) contain 20% of the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "WUUz-GURHjdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc3263a3-f64e-45ca-df3b-2456e0b63c41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 1453\n",
            "Validation set size: 485\n",
            "Test set size: 485\n"
          ]
        }
      ],
      "source": [
        "# Your code for the question above.\n",
        "# Split the dataset into training, validation, and test sets\n",
        "X = news_df['content']\n",
        "y = news_df['sentiment']\n",
        "\n",
        "# Splitting the dataset\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Print the sizes of each set\n",
        "print(\"Training set size:\", len(X_train))\n",
        "print(\"Validation set size:\", len(X_val))\n",
        "print(\"Test set size:\", len(X_test))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1rgu3ake1ZN"
      },
      "source": [
        "## Question 2: Establish Benchmarks\n",
        "\n",
        "In this question, we build two benchmark models: naive bayes and multinomial logistic regression, by using both 1-gram and 2-gram BOW representation. Note: `ngram_range=(1, 2)` means that the vectorizer will consider both unigrams (single words) and bigrams (pairs of consecutive words)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmWgt_KnxLOl"
      },
      "source": [
        "### a) Vectorization\n",
        "\n",
        "We will use the Count Vectorizer with n-grams and stopword removal. You can run the following code without making any edits. (If you run into errors, check to make sure that you have correctly defined `X_train`, `y_train`, `X_test` and `y_test` in Q1(d).)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "0Dd9GyxUqLxP"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english')\n",
        "X_train_vectors = vectorizer.fit_transform(X_train)\n",
        "X_test_vectors = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii3WNSm4FvZu"
      },
      "source": [
        "### b) Build a Naive-Bayes Model\n",
        "\n",
        "Please run the code below without any edits to train a Naive Bayes model with the training data. Then, display the performance measures of the Naive Bayes model using the`classification_report` function. Hint: For the code to display the performance measures using `classification_report`, refer to the Class 5 notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "UpPzIRhBxopQ",
        "outputId": "b51793d3-0719-4432-d8da-6b441343cac5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "model_nb = naive_bayes.MultinomialNB()\n",
        "model_nb.fit(X_train_vectors, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "3klcsh6CfH3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57e38c61-3bb1-4446-a114-9d45e980d637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============= Naive Bayes ==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.67      0.41      0.51        54\n",
            "     neutral       0.76      0.82      0.79       303\n",
            "    positive       0.52      0.52      0.52       128\n",
            "\n",
            "    accuracy                           0.69       485\n",
            "   macro avg       0.65      0.58      0.61       485\n",
            "weighted avg       0.69      0.69      0.69       485\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code for the question above.\n",
        "y_pred_nb = model_nb.predict(X_test_vectors)\n",
        "\n",
        "print(\"{:=^40s}\".format(\" Naive Bayes \"))\n",
        "print(classification_report(y_test, y_pred_nb, zero_division=0.0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieFcJqZCBlTz"
      },
      "source": [
        "### c) Build a Multinomial Logistic Regression Model\n",
        "\n",
        "Please run the code below without any edits to train a logistic regression model with the training data. Then, display the performance measures of the logistic regression model using the `classification_report` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "X9N5XRVYBlTz",
        "outputId": "f761a901-da04-420a-ceb8-cc134a523725"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "model_lr = LogisticRegression()\n",
        "model_lr.fit(X_train_vectors, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Ax_aSQQnfH3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d033f934-8f71-4346-e24d-6ef2d9cfe0bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Multinomial Logistic Regression =========\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.67      0.41      0.51        54\n",
            "     neutral       0.75      0.94      0.84       303\n",
            "    positive       0.77      0.45      0.56       128\n",
            "\n",
            "    accuracy                           0.75       485\n",
            "   macro avg       0.73      0.60      0.64       485\n",
            "weighted avg       0.75      0.75      0.73       485\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code for the question above.\n",
        "y_pred_lr = model_lr.predict(X_test_vectors)\n",
        "\n",
        "print(\"{:=^50s}\".format(\" Multinomial Logistic Regression \"))\n",
        "print(classification_report(y_test, y_pred_lr, zero_division=0.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wg-rgzdHv_E"
      },
      "source": [
        "## Question 3: Classifying with a Pre-trained BERT Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yeC-jn7fWar"
      },
      "source": [
        "Please run the code below without any edits to load the libraries for running the BERT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Jsxp9raXgZX8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, BertTokenizer, AdamW, pipeline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X_piPkJc4gc"
      },
      "source": [
        "### Base Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-f2zLaPe6ym"
      },
      "source": [
        "We will use the Sentiment-Analysis-BERT model, which is trained on a corpus of customer comments. This model can analyze user comments on products and extract the expressed sentiments as `positive`, `negative`, or `neutral`. Check more details here: https://huggingface.co/MarieAngeA13/Sentiment-Analysis-BERT.\n",
        "\n",
        "As you may notice, this model is not specifically designed for financial text. Therefore, we will fine-tune it for our task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "6tlFpSArIfAt"
      },
      "outputs": [],
      "source": [
        "# load the model\n",
        "bert = pipeline(model=\"MarieAngeA13/Sentiment-Analysis-BERT\", device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcfcjRuwfH3D"
      },
      "source": [
        "### a) Sentiment Analysis Using the Pre-trained BERT model\n",
        "\n",
        "Let us start by testing the pre-trained BERT model with two sample sentences. For each sample sentence below, find the predicted sentiment and its corresponding score from the BERT model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "x9OE_LVKf-qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01102e79-9d20-4a56-b084-f295801571dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'positive', 'score': 0.6859033703804016}]\n",
            "[{'label': 'negative', 'score': 0.6639607548713684}]\n"
          ]
        }
      ],
      "source": [
        "# Sample Sentences\n",
        "test_sample_1 = \"The tech stocks lead strong Q2 earnings.\"\n",
        "test_sample_2 = \"The financial sector took a hit today.\"\n",
        "\n",
        "# Write your code for the question below: (Hint: Use the bert pipeline to predict the sentiment of the test samples)\n",
        "print(bert(test_sample_1))\n",
        "print(bert(test_sample_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXq9Uu1Enh0E"
      },
      "source": [
        "### b) Evaluate the Pre-trained BERT model\n",
        "\n",
        "Next, evaluate the BERT model using the same testing set that we used for evaluating the Naive Bayes and Logistic Regression models. To do this, you will first need to compute the predictions of the BERT model for the testing set, then display the performance measures of the BERT model using the `classification_report` function.\n",
        "\n",
        "Hint: One way to save the predictions of the bert model on the text set is to use the code: `y_pred_bert = [pred[\"label\"] for pred in bert(X_test.to_list())]`. After you saved the `y_pred_bert`, you can then create a `classification_report` by using `y_pred_bert` and `y_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKzQw5avfWas",
        "outputId": "d21a821a-7fc3-4005-8683-199dafd95920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.58      0.41      0.48        54\n",
            "     neutral       0.69      0.90      0.78       303\n",
            "    positive       0.58      0.23      0.33       128\n",
            "\n",
            "    accuracy                           0.67       485\n",
            "   macro avg       0.62      0.51      0.53       485\n",
            "weighted avg       0.65      0.67      0.63       485\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code for the question above.\n",
        "# Compute the predictions of the BERT model for the testing set\n",
        "y_pred_bert = [pred[\"label\"] for pred in bert(X_test.to_list())]\n",
        "\n",
        "# Display the performance measures using classification_report\n",
        "print(classification_report(y_test, y_pred_bert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCQCLeXEfH3D"
      },
      "source": [
        "### c) Comparing the Three Models\n",
        "\n",
        "Compare the classification reports for the Naive Bayes, Logistic Regression, and Pre-trained BERT models. Based on the reports, which model seems to be the most effective? (Hint: While you can find all of the reports in the previous questions, it should be easier to print out all three classification reports together in a single block for comparison.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "OpoMCWeOfH3E"
      },
      "outputs": [],
      "source": [
        "# Optional Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ7y-x7SfH3E"
      },
      "source": [
        "Your answer to the question above:\n",
        "============= Naive Bayes ==============\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "    negative       0.67      0.41      0.51        54\n",
        "     neutral       0.76      0.82      0.79       303\n",
        "    positive       0.52      0.52      0.52       128\n",
        "\n",
        "    accuracy                           0.69       485\n",
        "   macro avg       0.65      0.58      0.61       485\n",
        "weighted avg       0.69      0.69      0.69       485\n",
        "\n",
        "======== Multinomial Logistic Regression =========\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "    negative       0.67      0.41      0.51        54\n",
        "     neutral       0.75      0.94      0.84       303\n",
        "    positive       0.77      0.45      0.56       128\n",
        "\n",
        "    accuracy                           0.75       485\n",
        "   macro avg       0.73      0.60      0.64       485\n",
        "weighted avg       0.75      0.75      0.73       485\n",
        "======== BERT =========\n",
        "            precision    recall  f1-score   support\n",
        "\n",
        "    negative       0.58      0.41      0.48        54\n",
        "     neutral       0.69      0.90      0.78       303\n",
        "    positive       0.58      0.23      0.33       128\n",
        "\n",
        "    accuracy                           0.67       485\n",
        "   macro avg       0.62      0.51      0.53       485\n",
        "weighted avg       0.65      0.67      0.63       485\n",
        "<br><br>\n",
        "Based on the provided classification reports, the Multinomial Logistic Regression model seems to be the most effective. It has the highest precision, recall, and f1-score for the majority of the classes. The precision, recall, and f1-score for the positive class are notably higher compared to the other two models. Additionally, the accuracy of the Multinomial Logistic Regression model is higher than the other models, indicating better overall performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjLXj4X8fH3E"
      },
      "source": [
        "## Q4) Finetuning the BERT Model\n",
        "\n",
        "Next, we will two types of finetuning of the BERT model to perform our classification tasks.\n",
        "\n",
        "First, run the code blocks below without edits to load the original MarieAngeA13 BERT model and prepare the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoLpI2Q6h8CC"
      },
      "source": [
        "### Model Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "eKXrT6qe6EX6"
      },
      "outputs": [],
      "source": [
        "bert_base = AutoModel.from_pretrained(\"MarieAngeA13/Sentiment-Analysis-BERT\")\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"MarieAngeA13/Sentiment-Analysis-BERT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3pi6kkkjfIB"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDM1UNtri1-V"
      },
      "source": [
        "We convert the sentiment names to integer indices for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGuha9rBMpds",
        "outputId": "9650384e-816e-4873-e28a-7073926a6668"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neutral': 0, 'positive': 1, 'negative': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "sentiment_list = news_df[\"sentiment\"].value_counts().index\n",
        "sentiment2int = dict(zip(sentiment_list, range(len(sentiment_list))))\n",
        "sentiment2int"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_od2sCbjowL"
      },
      "source": [
        "### Tokenize Data and Converting the Tokenized Data to PyTorch Tensors\n",
        "\n",
        "We tokenize and encode the text sequences, and then convert the sequences to PyTorch tensors so they can be processed in the BERT model. Please run the code without any edits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "ETs47LDXLAsm"
      },
      "outputs": [],
      "source": [
        "# tokenize and encode sequences\n",
        "tokens_train = bert_tokenizer.batch_encode_plus(\n",
        "    X_train.tolist(),\n",
        "    padding=True,\n",
        "    truncation=True)\n",
        "\n",
        "tokens_val = bert_tokenizer.batch_encode_plus(\n",
        "    X_val.tolist(),\n",
        "    padding=True,\n",
        "    truncation=True)\n",
        "\n",
        "tokens_test = bert_tokenizer.batch_encode_plus(\n",
        "    X_test.tolist(),\n",
        "    padding=True,\n",
        "    truncation=True)\n",
        "\n",
        "# convert lists to tensors\n",
        "train_seq = torch.tensor(tokens_train[\"input_ids\"])\n",
        "train_mask = torch.tensor(tokens_train[\"attention_mask\"])\n",
        "y_train_tensor = torch.tensor(y_train.map(sentiment2int).to_list())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val[\"input_ids\"])\n",
        "val_mask = torch.tensor(tokens_val[\"attention_mask\"])\n",
        "y_val_tensor = torch.tensor(y_val.map(sentiment2int).tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test[\"input_ids\"])\n",
        "test_mask = torch.tensor(tokens_test[\"attention_mask\"])\n",
        "y_test_tensor = torch.tensor(y_test.map(sentiment2int).tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMo3-16wk3_M"
      },
      "source": [
        "### a) Model Structure\n",
        "\n",
        "Next, we will specify the architure of our BERT model. The architure of the model is similar to the one in the Class 5 notebook. You may run the code below without any edits.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "nk12hzPj0aFh"
      },
      "outputs": [],
      "source": [
        "hidden_size_fc1 = 512\n",
        "\n",
        "class BERT_architecture(nn.Module):\n",
        "  def __init__(self, bert, n_class, freeze=True):\n",
        "    super(BERT_architecture, self).__init__()\n",
        "    self.bert = bert\n",
        "    self.n_class = n_class\n",
        "    if freeze:\n",
        "      # freeze the pretrained layers\n",
        "      for param in self.bert.parameters():\n",
        "        param.requires_grad = False\n",
        "    else:\n",
        "      # unfreeze the pretrained layers\n",
        "      for param in self.bert.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # add new layers\n",
        "    self.fc1 = nn.Linear(768,hidden_size_fc1)\n",
        "    self.dropout = nn.Dropout(p = 0.2)\n",
        "    self.relu =  nn.ReLU()\n",
        "    self.fc2 = nn.Linear(hidden_size_fc1, self.n_class)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  # set the forward pass\n",
        "  def forward(self, sent_id, mask):\n",
        "    _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "    x = self.fc1(cls_hs)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.softmax(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0TW48LgfH3F"
      },
      "source": [
        "### Q4 b) Model Hyperparameters\n",
        "\n",
        "Please answer each of the following questions with a single sentence.\n",
        "\n",
        "i) What happens if we change the code `self.fc1 = nn.Linear(768,hidden_size_fc1)` from above to `self.fc1 = nn.Linear(512,hidden_size_fc1)`?\n",
        "\n",
        "ii) What happens if we change the code `hidden_size_fc1 = 512` from above to `hidden_size_fc1 = 100`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRUnmdWEfH3F"
      },
      "source": [
        "Your answer to the question above:\n",
        "i) The input size of the first fully connected (FC) layer is reduced from 768 to 512.\n",
        "\n",
        "This change means that the FC layer will now expect a lower-dimensional input. 768 is wrong and will cause a mismatch between the input size expected by the FC layer and the actual input size. This can lead to errors during the model training or inference process.\n",
        "\n",
        "ii) The size of the hidden layer in the fully connected (FC) layers of the model is reduced from 512 to 100. The hidden size determines the number of neurons in the FC layer.\n",
        "\n",
        "The capacity of the FC layer to learn complex patterns in the data is reduced. This can result in a less expressive model that may struggle to capture intricate relationships between the input features and the output labels. The model's performance may decrease, especially if the original hidden size was chosen based on the complexity of the task or the size of the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLnLTxFqfH3F"
      },
      "source": [
        "### Q4 c) Other Model Architectures\n",
        "\n",
        "Suppose hypothetically, that we want to build a BERT model with three additional linear layers instead of two. How would you modify the code in the `BERT_architecture` class? Please note that you will need need to modify the following lines of code in the `BERT_architecture` class.\n",
        "\n",
        "```\n",
        "self.fc1 = nn.Linear(768,hidden_size_fc1)\n",
        "self.dropout = nn.Dropout(p = 0.2)\n",
        "self.relu =  nn.ReLU()\n",
        "self.fc2 = nn.Linear(hidden_size_fc1, self.n_class)\n",
        "self.softmax = nn.LogSoftmax(dim=1)\n",
        "```\n",
        "\n",
        "Please write your modified code in the text box below. DO NOT modify the actual code from part (a) and DO NOT run the modified code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps4VBi2afH3G"
      },
      "source": [
        "Your answer to the question above:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_architecture(nn.Module):\n",
        "  def __init__(self, bert, n_class, freeze=True):\n",
        "    super(BERT_architecture, self).__init__()\n",
        "    self.bert = bert\n",
        "    self.n_class = n_class\n",
        "    if freeze:\n",
        "      # freeze the pretrained layers\n",
        "      for param in self.bert.parameters():\n",
        "        param.requires_grad = False\n",
        "    else:\n",
        "      # unfreeze the pretrained layers\n",
        "      for param in self.bert.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        " # add new layers\n",
        "    self.fc1 = nn.Linear(768, hidden_size_fc1)\n",
        "    self.fc2 = nn.Linear(hidden_size_fc1, self.n_class)\n",
        "    hidden_size_fc2= self.n_class\n",
        "    self.fc3 = nn.Linear(hidden_size_fc2, self.n_class)\n",
        "    hidden_size_fc3= self.n_class\n",
        "    self.fc4 = nn.Linear(hidden_size_fc3, self.n_class)\n",
        "    self.dropout = nn.Dropout(p=0.2)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  # set the forward pass\n",
        "  def forward(self, sent_id, mask):\n",
        "    _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "    x = self.fc1(cls_hs)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc3(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc4(x)\n",
        "    x = self.softmax(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "s2RB2um_74n6"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zukpkVNlboY"
      },
      "source": [
        "### Q4 d) Training Setup\n",
        "\n",
        "Next, we provide code for various procedures (training procedure, evaluation procedure, set up data load, compute class weights) to get our BERT model ready for training. You may run all of the code below without modifications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKAo3UdLoS1j"
      },
      "source": [
        "Code for Training Procedure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "wQtxawMHO03R"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "def train(model, device):\n",
        "  model.train()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  # iterate over batches\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    # clear previously calculated gradients\n",
        "    model.zero_grad()\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    #optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    # push predictions back to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  # returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZrcuYWeocwf"
      },
      "source": [
        "Code for Evaluation Procedure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "-N696PgcO3fb"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, device):\n",
        "  print(\"Evaluating...\")\n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds, labels)\n",
        "      total_loss = total_loss + loss.item()\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader)\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  return avg_loss, total_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dadt0twkmGqL"
      },
      "source": [
        "Code for Set Up Data Loader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "t13miFnGPQEr"
      },
      "outputs": [],
      "source": [
        "# define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, y_train_tensor)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, y_val_tensor)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTH2xNk7mOiN"
      },
      "source": [
        "Code for compute weights for each sentiment for the data imbalance.\n",
        "\n",
        "The weight for any class `y` is calcualted by the formula:  ```n_samples / (n_classes * np.bincount(y))```. So the rarer classes (with less `np.bincount(y)`) will get higher weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxIZVgaHQFws",
        "outputId": "6ee514a6-e1cc-4d25-fbbd-9bb7bbf1a3a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Class Weights========\n",
            "neutral : 0.572\n",
            "positive : 1.156\n",
            "negative : 2.590\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# compute the class weights\n",
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train_tensor), y=np.asarray(y_train_tensor))\n",
        "print(\"{:=^30s}\".format(\" Class Weights\" ))\n",
        "print(*[\"{} : {:.3f}\".format(emotion, weight) for weight, emotion in zip(class_weights, sentiment2int.keys())], sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P63Z0jBOjdw_"
      },
      "source": [
        "### e) Training the Model while Freezing BERT Layer\n",
        "\n",
        "Now, let us finetune a model which freezes the BERT layer. The code below specifies the hyperparameters of the model and the training algorithm.\n",
        "It specificies all of the hyperparameters except declaring the optimizer for the training algorithm. Please declare the optimizer using AdamW as we did in the Class 5 notebook, and set the learning rate to 1e-4.\n",
        "\n",
        "Note: If you are interested, feel free to experiment with other learning rates and the number of epochs to see if it makes any difference, as these are two of the key training hyperparameters. Please be aware that each time you re-train the model, it will take some time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "tyrwim0cQYD2"
      },
      "outputs": [],
      "source": [
        "n_class = y_train.nunique()\n",
        "\n",
        "bert_finetune = BERT_architecture(bert_base, n_class=3)\n",
        "bert_finetune = bert_finetune.to(device)\n",
        "\n",
        "# wrap class weights into tensor\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "# push weights to GPU\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "\n",
        "# define loss function\n",
        "# add weights to handle the \"imbalance\" in the dataset\n",
        "cross_entropy  = nn.NLLLoss(weight=class_weights)\n",
        "\n",
        "# number of training epochs\n",
        "epochs = 10\n",
        "\n",
        "# Declare the optimizer for the training algorithm:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54hRpoLXpVTU"
      },
      "source": [
        "#### Model Training and Evaluation\n",
        "\n",
        "The following code will train the model in several epochs and save the model with the lowest loss on the evaluation dataset. The model weights will be stored in `bert_finetune_weights.pt`.\n",
        "\n",
        "You may run the code below without making any edits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8fFdDX1PEVJ",
        "outputId": "c6fa65be-d516-4231-ece4-25dcd1140790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 10\n",
            "Evaluating...\n",
            "Training Loss: 1.110\n",
            "Validation Loss: 1.090\n",
            "Epoch 2 / 10\n",
            "Evaluating...\n",
            "Training Loss: 1.104\n",
            "Validation Loss: 1.082\n",
            "Epoch 3 / 10\n",
            "Evaluating...\n",
            "Training Loss: 1.096\n",
            "Validation Loss: 1.071\n",
            "Epoch 4 / 10\n",
            "Evaluating...\n",
            "Training Loss: 1.087\n",
            "Validation Loss: 1.064\n",
            "Epoch 5 / 10\n",
            "Evaluating...\n",
            "Training Loss: 1.076\n",
            "Validation Loss: 1.057\n",
            "Epoch 6 / 10\n",
            "Evaluating...\n",
            "Training Loss: 1.078\n",
            "Validation Loss: 1.052\n",
            "Epoch 7 / 10\n",
            "Evaluating...\n",
            "Training Loss: 1.071\n",
            "Validation Loss: 1.047\n",
            "Epoch 8 / 10\n",
            "Evaluating...\n",
            "Training Loss: 1.074\n",
            "Validation Loss: 1.043\n",
            "Epoch 9 / 10\n",
            "Evaluating...\n",
            "Training Loss: 1.069\n",
            "Validation Loss: 1.039\n",
            "Epoch 10 / 10\n",
            "Evaluating...\n",
            "Training Loss: 1.056\n",
            "Validation Loss: 1.035\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(bert_finetune.parameters(), lr = 1e-5)\n",
        "\n",
        "best_valid_loss = float(\"inf\")\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "for epoch in range(epochs):\n",
        "  print(\"Epoch {:} / {:}\".format(epoch + 1, epochs))\n",
        "  # train model\n",
        "  train_loss, _ = train(bert_finetune, device)\n",
        "  # evaluate model\n",
        "  valid_loss, _ = evaluate(bert_finetune, device)\n",
        "  # save the best model\n",
        "  if valid_loss < best_valid_loss:\n",
        "    best_valid_loss = valid_loss\n",
        "    torch.save(bert_finetune.state_dict(), \"bert_finetune_weights.pt\")\n",
        "  # append training and validation loss\n",
        "  train_losses.append(train_loss)\n",
        "  valid_losses.append(valid_loss)\n",
        "  print(\"Training Loss: {:.3f}\".format(train_loss))\n",
        "  print(\"Validation Loss: {:.3f}\".format(valid_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUjJ5zX-paQY"
      },
      "source": [
        "The following code will load the stored model and evaluate it on the test data. Please run it without making any edits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "fC5NtuEHptvw"
      },
      "outputs": [],
      "source": [
        "bert_finetune = BERT_architecture(bert_base, n_class=3)\n",
        "bert_finetune.load_state_dict(torch.load(\"/content/bert_finetune_weights.pt\"))\n",
        "bert_finetune = bert_finetune.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHx3mZfaO7X_",
        "outputId": "d69048e0-dac2-4575-85d7-9e1a111d7bc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========== Fine-tune Added Layers Only===========\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral       0.70      0.83      0.76       303\n",
            "    positive       0.43      0.37      0.39       128\n",
            "    negative       0.67      0.19      0.29        54\n",
            "\n",
            "    accuracy                           0.64       485\n",
            "   macro avg       0.60      0.46      0.48       485\n",
            "weighted avg       0.62      0.64      0.61       485\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the performance of the finetuned model (while freezing the BERT layer) on the test set with classification_report\n",
        "with torch.no_grad():\n",
        "  y_pred_finetune = bert_finetune(test_seq.to(device), test_mask.to(device))\n",
        "  y_pred_finetune = y_pred_finetune.detach().cpu().numpy()\n",
        "y_pred_finetune = np.argmax(y_pred_finetune, axis = 1)\n",
        "print(\"{:=^50s}\".format(\" Fine-tune Added Layers Only\"))\n",
        "print(classification_report(y_test_tensor, y_pred_finetune, target_names=sentiment2int.keys(), zero_division=0.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4fPOHszeH1z"
      },
      "source": [
        "### f) Unfreeze the BERT Layer\n",
        "\n",
        "Next, let us unfreeze the BERT layer for model training. Please declare a `bert_unfreeze` model and update `optimizer` for the new model. Please keep the number of epochs to 10 and set the learning rate for the optimizer to 1e-4.\n",
        "(Hint: you can refer to the code in the Class 5 notebook, but you may need to make some minor modifications)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "vNI2UgkWfH3I"
      },
      "outputs": [],
      "source": [
        "# Your code for the question above:\n",
        "bert_unfreeze = BERT_architecture(bert_base, n_class=n_class, freeze=False)\n",
        "optimizer = torch.optim.AdamW(bert_unfreeze.parameters(), lr = 1e-5)  # learning rate\n",
        "bert_unfreeze = bert_unfreeze.to(device)\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK5pZshgsub-"
      },
      "source": [
        "Next, we provide code for training the `bert_unfreeze` model, Please run it without any edits. The model weights with the lowest evaluation loss will be stored in `bert_unfreeze_weights.pt`.\n",
        "\n",
        "Note: since we need to train both BERT and the added layer, it will take more time for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8XkaiXlv-A4",
        "outputId": "751b5268-3f9b-4f9b-daf3-3540b38085b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 10\n",
            "Evaluating...\n",
            "Training Loss: 1.136\n",
            "Validation Loss: 1.138\n",
            "Epoch 2 / 10\n",
            "Evaluating...\n",
            "Training Loss: 1.104\n",
            "Validation Loss: 1.112\n",
            "Epoch 3 / 10\n",
            "Evaluating...\n",
            "Training Loss: 1.075\n",
            "Validation Loss: 1.092\n",
            "Epoch 4 / 10\n",
            "Evaluating...\n",
            "Training Loss: 1.059\n",
            "Validation Loss: 1.062\n",
            "Epoch 5 / 10\n",
            "Evaluating...\n",
            "Training Loss: 1.016\n",
            "Validation Loss: 1.043\n",
            "Epoch 6 / 10\n",
            "Evaluating...\n",
            "Training Loss: 0.999\n",
            "Validation Loss: 1.032\n",
            "Epoch 7 / 10\n",
            "Evaluating...\n",
            "Training Loss: 0.982\n",
            "Validation Loss: 1.019\n",
            "Epoch 8 / 10\n",
            "Evaluating...\n",
            "Training Loss: 0.967\n",
            "Validation Loss: 1.005\n",
            "Epoch 9 / 10\n",
            "Evaluating...\n",
            "Training Loss: 0.946\n",
            "Validation Loss: 0.991\n",
            "Epoch 10 / 10\n",
            "Evaluating...\n",
            "Training Loss: 0.967\n",
            "Validation Loss: 0.988\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(bert_unfreeze.parameters(), lr = 1e-5)\n",
        "best_valid_loss = float(\"inf\")\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "  print(\"Epoch {:} / {:}\".format(epoch + 1, epochs))\n",
        "  # train model\n",
        "  train_loss, _ = train(bert_unfreeze, device)\n",
        "  # evaluate model\n",
        "  valid_loss, _ = evaluate(bert_unfreeze, device)\n",
        "  # save the best model\n",
        "  if valid_loss < best_valid_loss:\n",
        "    best_valid_loss = valid_loss\n",
        "    torch.save(bert_unfreeze.state_dict(), \"bert_unfreeze_weights.pt\")\n",
        "  # append training and validation loss\n",
        "  train_losses.append(train_loss)\n",
        "  valid_losses.append(valid_loss)\n",
        "  print(\"Training Loss: {:.3f}\".format(train_loss))\n",
        "  print(\"Validation Loss: {:.3f}\".format(valid_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcofSDGpfH3I"
      },
      "source": [
        "### g) Evaluating the `bert_unfreeze` Model\n",
        "\n",
        "Next, evaluate the `bert_unfreeze` model using the testing set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "L5AJVG12fH3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f839486d-701e-4dda-ce13-ce1e64bef15f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 10\n",
            "Evaluating...\n",
            "Training Loss: 0.942\n",
            "Validation Loss: 0.983\n",
            "Epoch 2 / 10\n",
            "Evaluating...\n",
            "Training Loss: 0.936\n",
            "Validation Loss: 0.978\n",
            "Epoch 3 / 10\n",
            "Evaluating...\n",
            "Training Loss: 0.903\n",
            "Validation Loss: 0.980\n",
            "Epoch 4 / 10\n",
            "Evaluating...\n",
            "Training Loss: 0.925\n",
            "Validation Loss: 0.971\n",
            "Epoch 5 / 10\n",
            "Evaluating...\n",
            "Training Loss: 0.912\n",
            "Validation Loss: 0.966\n",
            "Epoch 6 / 10\n",
            "Evaluating...\n",
            "Training Loss: 0.908\n",
            "Validation Loss: 0.964\n",
            "Epoch 7 / 10\n",
            "Evaluating...\n",
            "Training Loss: 0.923\n",
            "Validation Loss: 0.976\n",
            "Epoch 8 / 10\n",
            "Evaluating...\n",
            "Training Loss: 0.907\n",
            "Validation Loss: 0.954\n",
            "Epoch 9 / 10\n",
            "Evaluating...\n",
            "Training Loss: 0.897\n",
            "Validation Loss: 0.975\n",
            "Epoch 10 / 10\n",
            "Evaluating...\n",
            "Training Loss: 0.924\n",
            "Validation Loss: 0.968\n"
          ]
        }
      ],
      "source": [
        "# Your code for the question above:\n",
        "best_valid_loss = float(\"inf\")\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "  print(\"Epoch {:} / {:}\".format(epoch + 1, epochs))\n",
        "  # train model\n",
        "  train_loss, _ = train(bert_unfreeze, device)\n",
        "  # evaluate model\n",
        "  valid_loss, _ = evaluate(bert_unfreeze, device)\n",
        "  # save the best model\n",
        "  if valid_loss < best_valid_loss:\n",
        "    best_valid_loss = valid_loss\n",
        "    torch.save(bert_unfreeze.state_dict(), \"bert_unfreeze_weights.pt\")\n",
        "  # append training and validation loss\n",
        "  train_losses.append(train_loss)\n",
        "  valid_losses.append(valid_loss)\n",
        "  print(\"Training Loss: {:.3f}\".format(train_loss))\n",
        "  print(\"Validation Loss: {:.3f}\".format(valid_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHMQv0hKuREF"
      },
      "source": [
        "### h) Comparing the Models\n",
        "\n",
        "Briefly comment on whether the `bert_unfreeze` model is more effective than the logistic regression benchmark."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  y_pred_unfreeze = bert_unfreeze(test_seq.to(device), test_mask.to(device))\n",
        "  y_pred_unfreeze = y_pred_unfreeze.detach().cpu().numpy()\n",
        "y_pred_unfreeze = np.argmax(y_pred_unfreeze, axis = 1)\n",
        "print(\"{:=^50s}\".format(\" Finetune BERT and Added Layers \"))\n",
        "print(classification_report(y_test_tensor, y_pred_unfreeze, zero_division=0.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljvW96gRscV1",
        "outputId": "bced2fc2-febc-494e-89dd-3d1b482de39d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========= Finetune BERT and Added Layers =========\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       303\n",
            "           1       0.30      1.00      0.46       128\n",
            "           2       0.75      0.74      0.75        54\n",
            "\n",
            "    accuracy                           0.35       485\n",
            "   macro avg       0.35      0.58      0.40       485\n",
            "weighted avg       0.16      0.35      0.20       485\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your Answer for the question above:\n",
        "\n",
        "========= Finetune BERT and Added Layers =========\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.62      1.00      0.77       303\n",
        "           1       0.00      0.00      0.00       128\n",
        "           2       0.00      0.00      0.00        54\n",
        "\n",
        "    accuracy                           0.62       485\n",
        "   macro avg       0.21      0.33      0.26       485\n",
        "weighted avg       0.39      0.62      0.48       485\n",
        "======== Multinomial Logistic Regression =========\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "    negative       0.67      0.41      0.51        54\n",
        "     neutral       0.75      0.94      0.84       303\n",
        "    positive       0.77      0.45      0.56       128\n",
        "\n",
        "    accuracy                           0.75       485\n",
        "   macro avg       0.73      0.60      0.64       485\n",
        "weighted avg       0.75      0.75      0.73       485\n",
        "\n",
        "It looks like Multinomial Logistic Regression is still better than BERT unfreeze. one thing we noticed is the recall and F1 score for neutral are significantly higher than Logistics regression. That is probably because of the weights we put in."
      ],
      "metadata": {
        "id": "CKyZhOt-tUbs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcAvHz_W0w_x"
      },
      "source": [
        "## Bonus Question: FinBERT\n",
        "\n",
        "Recall that FinBERT is a BERT model pre-trained on financial corpus, and we have used it in the `PreClass2_Evaluation` notebook. There are actually multiple pre-trained FinBERT models. In this notebook, we will use the FinBERT model from \"ProsusAI/finbert\".\n",
        "\n",
        "The code below downloads the FinBERT model.\n",
        "\n",
        "### Bonus Part a)\n",
        "\n",
        "Please add the necessary codes to evaluate the pre-trained FinBERT model from \"ProsusAI/finbert\" using the classification_report function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "-Ms3fN3XfH3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddab3152-3022-4c76-8750-f09e28845a1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.75      0.98      0.85        54\n",
            "     neutral       0.95      0.81      0.88       303\n",
            "    positive       0.72      0.88      0.79       128\n",
            "\n",
            "    accuracy                           0.85       485\n",
            "   macro avg       0.81      0.89      0.84       485\n",
            "weighted avg       0.87      0.85      0.85       485\n",
            "\n"
          ]
        }
      ],
      "source": [
        "finbert = pipeline(model=\"ProsusAI/finbert\", device=device)\n",
        "\n",
        "# Your Code:\n",
        "\n",
        "# Compute the predictions of the FinBERT model for the testing set\n",
        "y_pred_finbert = [pred[\"label\"] for pred in finbert(X_test.tolist())]\n",
        "\n",
        "# Display the performance measures of the FinBERT model using classification_report\n",
        "print(classification_report(y_test, y_pred_finbert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it9KbAvB3KHE"
      },
      "source": [
        "### Bonus Part b)\n",
        "\n",
        "If you completed part a) correctly, you should see that the FinBERT model has very strong performance. Can you identify some possible reasons for this strong performance based on the model description? (https://huggingface.co/ProsusAI/finbert)\n",
        "\n",
        "Also, if we apply the model to future financial news articles, do you expect the FinBERT model to have the same effectiveness as it exhibited in the testing set?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbzSdA8bfH3K"
      },
      "source": [
        "Your answer to the question above:\n",
        "FINBERT is pre-trained on a large corpus of financial text. It performs better in financial data analysis.\n",
        "   \n",
        "FinBERT is fine-tuned on financial sentiment analysis, which can lead to improved performance compared to more general-purpose models like Logistic Regression or BERT.  \n",
        "   \n",
        "FinBERT utilizes contextual word embeddings.  \n",
        "   \n",
        "FinBERT weighs the relevance of different words in determining sentiment, which can enhance its performance in sentiment analysis tasks.  \n",
        "\n",
        "Yes, if we apply the model to future financial news articles, do you expect the FinBERT model to have the same effectiveness as it exhibited in the testing set?\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}